# C1 Northstar Technical Analysis

_Generated by analyzing .references/c1-rsf-app and .references/c1-e4e-app_

---

## 1. Database Schema & Data Models

### RSF Prisma Schema

```prisma
// This is your Prisma schema file,
// learn more about it in the docs: https://pris.ly/d/prisma-schema

// Looking for ways to speed up your queries, or scale easily with your serverless or edge functions?
// Try Prisma Accelerate: https://pris.ly/cli/accelerate-init

generator client {
  provider = "prisma-client-js"
}

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

model Product {
  id                       String    @id @default(uuid())
  itemNumber               String    @unique @map("item_number")
  itemDescription          String?   @map("item_description")
  itemTypeCode             Int?      @map("item_type_code")
  itemTypeDescription      String?   @map("item_type_description")
  productType              String?   @map("product_type")
  itemRevenueCategory      String?   @map("item_revenue_category")
  itemManufacturer         String?   @map("item_manufacturer")
  itemCategory             String?   @map("item_category")
  itemLineOfBusiness       String?   @map("item_line_of_business")
  itemSubcategory          String?   @map("item_subcategory")
  itemClass                String?   @map("item_class")
  portfolio                String?   @map("portfolio")
  currentCost              Float?    @map("current_cost")
  scdStartDate             DateTime? @map("scd_start_date")
  scdEndDate               DateTime? @map("scd_end_date")
  isCurrentRecordFlag      Boolean   @map("is_current_record_flag")
  offer                    String?   @map("offer")
  practice                 String?   @map("practice")
  solutionSegment          String?   @map("solution_segment")
  businessSegment          String?   @map("business_segment")
  manufacturerPractice     String?   @map("manufacturer_practice")
  manufacturerItemCategory String?   @map("manufacturer_item_category")
  growthCategory           String?   @map("growth_category")

  PurchaseProduct PurchaseProduct[]
}

model Engineer {
  id    String @id @default(uuid())
  name  String @map("employee_name")
  email String @unique @map("employee_email")

  qualifications EngineerQualification[]

  @@index([email])
}

model EngineerQualification {
  id               String  @id @default(uuid())
  vendor           String? @map("vendor")
  qualification    String? @map("qualification_names")
  expirationStatus String? @map("expiration_status")
  status           String? @map("employee_status")

  engineerId String   @map("engineer_id")
  engineer   Engineer @relation(fields: [engineerId], references: [id])

  @@index([engineerId])
}

model Account {
  id                         String        @id @default(uuid())
  gemStatus                  String?       @map("gem_status")
  accountName                String        @map("account_name")
  accountNumber              String        @unique @map("account_number")
  timelineInfo               String?       @map("timeline_info") // Remove this column from the schema later
  crmOwner                   String?       @map("crm_owner")
  cxSeatRange                String?       @map("cx_seat_range")
  targetSolutions            String?       @map("target_solutions")
  battleCardNotes            String?       @map("battle_card_notes")
  gemIndex                   String?       @map("gem_index")
  competitorResearch         String?       @map("competitor_research")
  ccIntent                   String?       @map("cc_intent")
  ccVendor                   String?       @map("cc_vendor")
  ucIntent                   String?       @map("uc_intent")
  ucVendor                   String?       @map("uc_vendor")
  dcIntent                   String?       @map("dc_intent")
  dcVendor                   String?       @map("dc_vendor")
  enIntent                   String?       @map("en_intent")
  enVendor                   String?       @map("en_vendor")
  sxIntent                   String?       @map("sx_intent")
  sxVendor                   String?       @map("sx_vendor")
  employeeId                 String?       @map("employee_id")
  customerSummit             String?       @map("customer_summit")
  ownerName                  String?       @map("owner_name")
  managerName                String?       @map("manager_name")
  newOrgSD                   String?       @map("new_org_sd")
  dso                        String?       @map("dso")
  programCategory            String?       @map("program_category")
  finalCustomerSegment       String?       @map("final_customer_segment")
  recommendedSolution        String?       @map("recommended_solution")
  ceCustomerSegment          String?       @map("ce_customer_segment")
  activeUCRegistration       String?       @map("active_uc_registration")
  ucSeats                    String?       @map("uc_seats")
  currentPrimaryUCPlatform   String?       @map("current_primary_uc_platform")
  otherUCPlatforms           String?       @map("other_uc_platforms")
  currentAvayaRelease        String?       @map("current_avaya_release")
  activeCCRegistration       String?       @map("active_cc_registration")
  ccSeats                    String?       @map("cc_seats")
  currentCCPlatform          String?       @map("current_cc_platform")
  otherCCPlatform            String?       @map("other_cc_platform")
  customerMeeting2025        String?       @map("customer_meeting_2025")
  highestRelationshipLevel   String?       @map("highest_relationship_level")
  normalizedSummary          String?       @map("normalized_summary")
  normalizedSummaryCreatedAt DateTime?     @map("normalized_summary_created_at")
  opportunities              Opportunity[]
}

model Opportunity {
  id                   String            @id @default(uuid())
  opportunityNumber    String            @unique @map("opportunity_number")
  customerName         String            @map("customer_name")
  oppStage             String?           @map("opp_stage")
  salesPerson          String?           @map("sales_person")
  salesDirector        String?           @map("sales_director")
  bookedDate           DateTime?         @map("booked_date")
  estimatedCloseDate   DateTime?         @map("estimated_close_date")
  bookedGrossRevenue   Float             @default(0.0) @map("booked_gross_revenue")
  pipelineGrossRevenue Float             @default(0.0) @map("pipeline_gross_revenue")
  margin               Float             @default(0.0) @map("margin")
  accountId            String            @map("account_id")
  Account              Account           @relation(fields: [accountId], references: [id])
  purchaseProducts     PurchaseProduct[]

  @@index([opportunityNumber])
}

model PurchaseProduct {
  id                 String      @id @default(uuid())
  gpRevenueCategory  String?     @map("gp_revenue_category")
  mappedSolutionArea String?     @map("mapped_solution_area")
  mappedSegment      String?     @map("mapped_segment")
  mappedCapability   String?     @map("mapped_capability")
  itemCategory       String?     @map("item_category")
  opportunityId      String      @map("opportunity_id")
  productId          String      @map("product_id")
  opportunity        Opportunity @relation(fields: [opportunityId], references: [id])
  product            Product     @relation(fields: [productId], references: [id])

  @@index([opportunityId])
  @@index([productId])
}

enum PlatformType {
  CONTACT_CENTER
  UNIFIED_COMMUNICATIONS
  DATA_CENTER
  ENTERPRISE_NETWORKING
  SOFTWARE
}

enum PairingType {
  PREFERRED
  SECONDARY
}

model CXPlatformOffering {
  id               String       @id @default(uuid())
  platform         String       @map("platform")
  platformType     PlatformType @map("platform_type")
  product          String       @map("product")
  features         String[]     @map("features")
  scalabilityMin   String       @default("0") @map("scalability_min")
  scalabilityMax   String       @default("0") @map("scalability_max")
  primaryTarget    String       @map("primary_target")
  c1FocusResale    Boolean      @default(false) @map("c1_focus_resale")
  c1FocusPS        Boolean      @default(false) @map("c1_focus_ps")
  c1FocusMS        Boolean      @default(false) @map("c1_focus_ms")
  c1FocusMSDetails String?      @map("c1_focus_ms_details")
  createdAt        DateTime     @default(now()) @map("created_at")
  updatedAt        DateTime     @updatedAt @map("updated_at")
  ucPairings       CXPairing[]  @relation("UCProductPairings")
  ccPairings       CXPairing[]  @relation("CCProductPairings")
}

model CXPairing {
  id          String             @id @default(uuid())
  pairingType PairingType        @map("pairing_type")
  focus       Boolean            @default(false) @map("focus")
  ucProductId String             @map("uc_product_id")
  ccProductId String             @map("cc_product_id")
  ucProduct   CXPlatformOffering @relation("UCProductPairings", fields: [ucProductId], references: [id])
  ccProduct   CXPlatformOffering @relation("CCProductPairings", fields: [ccProductId], references: [id])

  @@unique([ucProductId, ccProductId])
}
```

### Current Northstar Schema

```prisma
// This is your Prisma schema file,
// learn more about it in the docs: https://pris.ly/d/prisma-schema

generator client {
  provider = "prisma-client-js"
}

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

// NextAuth.js models
model User {
  id            String          @id @default(cuid())
  name          String?
  email         String          @unique
  emailVerified DateTime?
  image         String?
  accounts      Account[]
  sessions      Session[]
  oid           String?         @unique // Microsoft Entra ID Object ID

  createdAt     DateTime        @default(now())
  updatedAt     DateTime        @updatedAt

  @@index([email])
  @@index([oid])
}

model Account {
  userId            String
  type              String
  provider          String
  providerAccountId String
  refresh_token     String?
  access_token      String?
  expires_at        Int?
  token_type        String?
  scope             String?
  id_token          String?
  session_state     String?

  createdAt DateTime @default(now())
  updatedAt DateTime @updatedAt

  user User @relation(fields: [userId], references: [id], onDelete: Cascade)

  @@id([provider, providerAccountId])
}

model Session {
  sessionToken String   @unique
  userId       String
  expires      DateTime
  user         User     @relation(fields: [userId], references: [id], onDelete: Cascade)

  createdAt DateTime @default(now())
  updatedAt DateTime @updatedAt
}

model VerificationToken {
  identifier String
  token      String
  expires    DateTime

  @@id([identifier, token])
}
```

### Key TypeScript Types (from RSF)

```typescript
// From .references/c1-rsf-app/src/types/account.ts
import { Account, Prisma } from "@prisma/client";

export type TAccount = Account;

export type TOpportunityWithProducts = Prisma.OpportunityGetPayload<{
  include: { purchaseProducts: { include: { product: true } } };
}>;

export type TScoredEntity = {
  name: string;
  totalAmount: number;
  count: number;
};

export type TCreateAccountParams = Omit<Account, "id" | "createdAt" | "updatedAt">;

export type TAccountOpportunitiesWithProducts = Prisma.AccountGetPayload<{
  include: { opportunities: { include: { purchaseProducts: { include: { product: true } } } } };
}>;

export type TUpdateAccountNormalizedSummaryParams = {
  accountNumber: string;
  normalizedSummary: string;
};

// From .references/c1-rsf-app/src/types/asset.ts
export type TAssetRow = {
  Name: string;
  Solution?: string;
  Offering?: string;
  "Asset Type"?: string;
};

export type TParsedBlock = {
  title?: string;
  bullets?: string[]; // Paragraphs, bullet points, table rows, etc.
  slideNumber?: number; // Optional â€” for PPTX
  sectionNumber?: number; // Optional â€” for DOCX/PDF/XLSX
};

export type TNormalizedChunk = {
  content: string;
  originalText: string;
  chunkIndex: number;
  tokenCount: number;
  contentHash: string;
  sourceFileName: string;
  sourceSection?: string;
  slideNumber?: number;
  documentCategory?: string;
  fileType: string;
  scope: string;
  normalized?: boolean;
};

// From .references/c1-rsf-app/src/types/opportunity.ts
import { Opportunity } from "@prisma/client";

export type TOpportunityCreateParams = Omit<Opportunity, "id" | "createdAt" | "updatedAt">;

export type TOpportunity = Opportunity;

// From .references/c1-rsf-app/src/types/message.ts
export type TAIMessage = {
  role: "system" | "user" | "assistant";
  content: string;
};

// From .references/c1-rsf-app/src/types/vector.ts
import { Schemas } from "@qdrant/js-client-rest";

export type TVector = number[];
export type TVectorFilter = Schemas["Filter"];
export type TVectorMustFilter = TVectorFilter["must"];
export type TVectorSearchScoredPoint = Schemas["ScoredPoint"];

export type TVectorMetadataValue = string | number | boolean | null | Date | TVectorMetadataValue[];

export type TVectorMetadata = Record<string, TVectorMetadataValue>;

export type TVectorScope = "sales-assets";

export type TVectorSourceType = "account-summary" | "account-details" | "global-context";

export type TVectorPayload = {
  content: string;
  contentHash: string;
  metadata: TVectorMetadata;
};

export type TVectorCreateParams = {
  collection: string;
  vector: TVector;
  payload: TVectorPayload;
};

export type TRelevantChunk = {
  id: string;
  score: number;
};

export type TEvaluationQueryBenchmark = {
  query: string;
  k: number;
  scope: string;
  relevantChunkIds: string[];
};

export type TEvaluationResult = {
  query: string;
  k: number;
  recallAtK: number;
  mrr: number;
  retrievedIds: string[];
  relevantChunks: TRelevantChunk[];
};

export type TRankedVectorResult = {
  id: string;
  score: number;
  content: string;
  chunkId: string;
  payload: TVectorMetadata;
};
```

### DTOs (from RSF)

```typescript
// From .references/c1-rsf-app/src/application/dto/account.dto.ts
import { TAccount, TOpportunity, TScoredEntity } from "@/types";

export type TAccountInsightData = {
  accountNumber: string;
  industry?: string;
  gemStatus?: string;
  focusSolution?: string;
  opportunityCount: number;
  pipelineValue: number;
  bookedRevenue: number;
  recentWins?: string[];
  risks?: string[];

  // Raw full data
  account: TAccount;
  opportunities: TOpportunity[];
  recentOpportunities: TOpportunity[];

  // Aggregates
  topVendors: TScoredEntity[];
  topProducts: TScoredEntity[];
  totalRevenue: number;
};

export type TAccountSummaryNormalizedData = TAccountInsightData & {
  normalized: string;
  originalText: string;
};

// From .references/c1-rsf-app/src/application/dto/asset.dto.ts
export type TAssetScope = "sales-assets" | "proposals" | "training" | "other";

export type TAssetRowResult = {
  title: string;
  sourceFile: string;
  solutions: string[];
  offerings: string[];
  assetTypes: string[];
};

export type TDocumentMetadata = {
  fileName: string;
  title?: string;
  fileType: "pptx" | "pdf" | "docx" | "xlsx";
  documentCategory?: string; // e.g., "Case Study", "Data Sheet"
  solutions?: string[];
  offerings?: string[];
  assetTypes?: string[];
  audience?: string; // e.g., "External"
  industry?: string;
  program?: string;
  sourcePath?: string;
  scope: TAssetScope;
};

// From .references/c1-rsf-app/src/application/dto/insight.dto.ts
export type TInsight = {
  accountNumber: string;
  next_steps: string[];
  customer_ecosystem: string[];
  customer_value: string[];
  customer_interests: string[];
  customer_satisfaction: string[];
};

export type TExportInsight = {
  accountNumber: string;
  next_steps: [string, string, string];
  customer_interests: [string, string, string];
  customer_value: [string, string, string];
  customer_ecosystem: [string, string, string];
};

export type TInsightCategory =
  | "customer_interests"
  | "customer_ecosystem"
  | "customer_value"
  | "next_steps";

export interface InsightResultDTO {
  accountNumber: string;
  generatedAt: Date;
  categories: Record<TInsightCategory, string[]>;
}
```

---

## 2. CLI Commands & Processing Pipeline

### Available CLI Scripts

```json
{
  "scripts": {
    "start": "tsx src/presentation/cli/main.ts",
    "dev": "tsx watch src/presentation/cli/main.ts",
    "lint": "eslint . --ext .ts",
    "format": "prettier --write .",
    "generate:insights": "tsx src/presentation/cli/generate-insight-batch.ts",
    "generate:insight-single": "tsx src/presentation/cli/generate-insight-single.ts",
    "import:accounts": "tsx src/presentation/cli/import-accounts.ts",
    "import:products": "tsx src/presentation/cli/import-products.ts",
    "import:assets": "tsx src/presentation/cli/import-assets.ts",
    "import:opportunities": "tsx src/presentation/cli/import-opportunities.ts",
    "chunk:accounts": "tsx src/presentation/cli/chunk-accounts.ts",
    "chunk:account-summaries": "tsx src/presentation/cli/chunk-account-summaries.ts",
    "chunk:opportunity-details": "tsx src/presentation/cli/chunk-opportunity-details.ts",
    "chunk:assets": "tsx src/presentation/cli/chunk-assets.ts",
    "validate:assets": "tsx src/presentation/cli/validate-import-assets.ts",
    "compare:normalizations": "tsx src/presentation/cli/compare-normalizations.ts",
    "evaluate:query-benchmarks": "tsx src/presentation/cli/evaluate-query-benchmarks.ts",
    "compare:normalizations-by-chunk-id": "tsx src/presentation/cli/compare-normalizations-by-chunk-id.ts"
  }
}
```

### Import Use Cases

```typescript
// From .references/c1-rsf-app/src/application/use-cases/import/import-accounts.usecase.ts
import { AccountLoaderService } from "@/infrastructure/import/account-loader.service";
import { ProgressTracker } from "@/shared/progress-tracker";
import { TPrismaClient } from "@/types";

export class ImportAccountsUseCase {
  constructor(
    private readonly db: TPrismaClient,
    private readonly loader = new AccountLoaderService(),
  ) {}

  async run(progress?: ProgressTracker): Promise<void> {
    const map = await this.loader.loadMergedAccounts();

    let success = 0;
    let failed = 0;

    progress?.status("ğŸ”„ Starting account import...");
    progress?.increment(0); // kick off progress display

    for (const account of map.values()) {
      try {
        await this.db.account.upsert({
          where: { accountNumber: account.accountNumber },
          create: account,
          update: account,
        });

        success++;
      } catch (err) {
        console.error(`âŒ Failed to import ${account.accountNumber}`, err);
        failed++;
      } finally {
        progress?.increment();
      }
    }

    progress?.succeed(`âœ… Accounts imported: ${success}`);
    if (failed) console.log(`âš ï¸ Accounts failed: ${failed}`);
  }
}

// From .references/c1-rsf-app/src/application/use-cases/import/import-assets.usecase.ts
import fs from "node:fs/promises";
import path from "node:path";

import { TDocumentMetadata } from "@/application/dto/asset.dto";
import { GlobalVectorChunkBuilder } from "@/application/use-cases/chunk/global-vector-chunk-builder.usecase";
import { NormalizedAssetFormatter } from "@/infrastructure/formatters/normalized-asset.formatter";
import { AssetLoaderService } from "@/infrastructure/import/asset-loader.service";
import { FileParserService } from "@/infrastructure/import/file-parser.service";
import { DocumentNormalizerService } from "@/infrastructure/normalizers/document-normalizer.service";
import { RecursiveCharacterTextSplitter } from "@/shared/recursive-splitter";
import { TVectorPayload } from "@/types/vector";

export class ImportAssetsUseCase {
  constructor(
    private readonly assetLoader = new AssetLoaderService(),
    private readonly documentNormalizer = new DocumentNormalizerService(),
    private readonly fileParser = new FileParserService(),
    private readonly chunkBuilder = new GlobalVectorChunkBuilder(),
    private readonly formatter = new NormalizedAssetFormatter(),
    private readonly splitter = new RecursiveCharacterTextSplitter(400, 200),
  ) {}

  async run(params: { assetFilePath: string; assetFolderPath: string }): Promise<void> {
    const { assetFilePath, assetFolderPath } = params;

    const rows = await this.assetLoader.load(assetFilePath);

    for (const row of rows) {
      const assetPath = path.resolve(assetFolderPath, row.sourceFile);
      const fileExt = path.extname(assetPath);
      if (fileExt === ".xlsx" || fileExt === ".xls") {
        console.warn(
          `[ImportAssetsUseCase] Skipping ${row.sourceFile}: Excel files are not supported`,
        );
        continue;
      }

      try {
        await fs.access(assetPath);

        const rawContent = await this.fileParser.parse(assetPath);

        const metadata: TDocumentMetadata = {
          fileName: row.sourceFile,
          fileType: this.detectFileType(row.sourceFile),
          title: row.title,
          solutions: row.solutions,
          offerings: row.offerings,
          assetTypes: row.assetTypes,
          scope: "sales-assets",
        };

        const normalizedChunks = await this.documentNormalizer.normalize({ rawContent, metadata });

        const formattedPayloads: TVectorPayload[] = this.formatter.format({
          chunks: normalizedChunks,
          metadata,
        });

        const chunkedPayloads = await this.splitter.splitDocuments(formattedPayloads);

        await this.chunkBuilder.run(chunkedPayloads);

        await this.logAudit({
          sourceFile: row.sourceFile,
          fileType: metadata.fileType,
          totalParsedBlocks: rawContent.length,
          totalNormalizedChunks: normalizedChunks.length,
          totalSplitChunks: chunkedPayloads.length,
          totalEmbeddedChunks: chunkedPayloads.length, // assuming all are embedded successfully
        });
      } catch (err) {
        console.warn(
          `[ImportAssetsUseCase] Skipping ${row.sourceFile}:`,
          err instanceof Error ? err.message : String(err),
        );
        // Optionally log to file
      }
    }
  }

  private detectFileType(fileName: string): TDocumentMetadata["fileType"] {
    const ext = path.extname(fileName).toLowerCase();
    if (ext.includes("ppt")) return "pptx";
    if (ext.includes("pdf")) return "pdf";
    if (ext.includes("doc")) return "docx";
    if (ext.includes("xls")) return "xlsx";
    throw new Error(`Unsupported file type: ${ext}`);
  }

  private async logAudit(params: {
    sourceFile: string;
    fileType: string;
    totalParsedBlocks: number;
    totalNormalizedChunks: number;
    totalSplitChunks: number;
    totalEmbeddedChunks: number;
  }) {
    const folder = path.resolve("data/logs/import/assets");
    await fs.mkdir(folder, { recursive: true });

    const logPath = path.join(folder, "asset-import.jsonl");

    const entry = {
      timestamp: new Date().toISOString(),
      ...params,
    };

    await fs.appendFile(logPath, JSON.stringify(entry) + "\n");
  }
}

// From .references/c1-rsf-app/src/application/use-cases/import/import-products.usecase.ts
import { ProductLoaderService } from "@/infrastructure/import/product-loader.service";
import { ProgressTracker } from "@/shared/progress-tracker";
import { TPrismaClient } from "@/types";

export class ImportProductsUseCase {
  constructor(private readonly prisma: TPrismaClient) {}

  async run(): Promise<void> {
    const loader = new ProductLoaderService();
    const products = await loader.loadAll();

    const BATCH = 10000;
    const progress = new ProgressTracker("Importing products", Math.ceil(products.length / BATCH));
    progress.start();

    let success = 0;
    let failure = 0;

    for (let i = 0; i < products.length; i += BATCH) {
      const chunk = products.slice(i, i + BATCH);
      try {
        await this.prisma.product.createMany({
          data: chunk,
          skipDuplicates: true,
        });
        success += chunk.length;
      } catch (err) {
        console.error("âŒ Product batch failed:", err);
        failure += chunk.length;
      }
      progress.increment();
    }

    progress.succeed(`Products imported: ${success}, failed: ${failure}`);
  }
}

// From .references/c1-rsf-app/src/application/use-cases/import/import-opportunities.usecase.ts
import { OpportunityLoaderService } from "@/infrastructure/import/opportunity-loader.service";
import { AccountDataService } from "@/infrastructure/persistence/account-data.prisma.service";
import { ProgressTracker } from "@/shared/progress-tracker";
import { TPrismaClient } from "@/types";

export class ImportOpportunitiesUseCase {
  constructor(
    private readonly db: TPrismaClient,
    private readonly loader = new OpportunityLoaderService(),
    private readonly accountData = new AccountDataService(),
  ) {}

  async run(): Promise<void> {
    const accounts = await this.db.account.findMany();
    const products = await this.db.product.findMany();

    const { opportunities, purchaseItems } = await this.loader.loadPurchases({
      accounts,
      products,
    });

    const tracker = new ProgressTracker(
      "Importing opportunities",
      opportunities.size + purchaseItems.length,
    );
    tracker.start();

    tracker.status(`ğŸ” Upserted ${opportunities.size} opportunities`);

    const oppIdMap = new Map<string, string>(); // opportunityNumber â†’ id

    for (const [oppNumber, oppData] of opportunities) {
      try {
        const result = await this.db.opportunity.upsert({
          where: { opportunityNumber: oppNumber },
          create: oppData,
          update: oppData,
        });
        oppIdMap.set(oppNumber, result.id);
      } catch (err) {
        console.error(`âŒ Failed to upsert opportunity ${oppNumber}`, err);
      }
      tracker.increment();
    }

    tracker.status(`ğŸ”— Linking ${purchaseItems.length} products to opportunities...`);

    for (const item of purchaseItems) {
      const opportunityId = oppIdMap.get(item.opportunityNumber);
      if (!opportunityId) {
        console.warn(`âš ï¸ Skipped purchase item â€” unknown opportunity: ${item.opportunityNumber}`);
        tracker.increment();
        continue;
      }

      try {
        await this.db.purchaseProduct.create({
          data: {
            opportunityId,
            productId: item.productId,
            gpRevenueCategory: item.gpRevenueCategory,
            mappedSolutionArea: item.mappedSolutionArea,
            mappedSegment: item.mappedSegment,
            mappedCapability: item.mappedCapability,
            itemCategory: item.itemCategory,
          },
        });
      } catch (err) {
        console.error(`âŒ Failed to insert PurchaseProduct for ${item.opportunityNumber}`, err);
      }

      tracker.increment();
    }

    tracker.succeed("âœ… Opportunity + product imports complete");
  }
}
```

### Insight Generation Use Case

```typescript
// From .references/c1-rsf-app/src/application/use-cases/insight/generate-insights.usecase.ts
import { TAccountSummaryNormalizedData } from "@/application/dto/account.dto";
import { InsightResultDTO } from "@/application/dto/insight.dto";
import { ILanguageModelService } from "@/application/interfaces/language-model.service.interface";
import { extractSingleInsightSection } from "@/infrastructure/formatters/generate-insight.formatter";
import { InsightLLMService } from "@/infrastructure/llm/insight-llm.service";
import { LoggerService } from "@/infrastructure/log/logger.service";
import { QdrantVectorSearchService } from "@/infrastructure/vector/vector-search.qdrant.service";
import { config } from "@/shared/config";
import {
  buildCustomerEcosystemPrompt,
  buildCustomerInterestsPrompt,
  buildCustomerValuePrompt,
  buildNextStepsPrompt,
} from "@/shared/prompts/insights/insight-prompt-builder";
import { estimateTokenCount } from "@/shared/utils/token-utils";

type TGenerateInsightParams = {
  accountData: TAccountSummaryNormalizedData;
  memoryContext?: string;
  debug?: boolean;
};

export class GenerateInsightUseCase {
  constructor(
    private readonly vectorSearch = new QdrantVectorSearchService(),
    private readonly llm: ILanguageModelService = new InsightLLMService(),
    private readonly logger = new LoggerService("insight", "generate-insights.json"),
  ) {}

  async run(params: TGenerateInsightParams): Promise<InsightResultDTO> {
    const { accountData, memoryContext } = params;

    const textToChunk = accountData.originalText + "\n\n" + accountData.normalized;

    const { chunks } = await this._fetchChunks(accountData.accountNumber, textToChunk);

    const [customerEcosystem, customerValue, customerInterests] = await Promise.all([
      this._generateCustomerEcosystem(chunks, memoryContext),
      this._generateCustomerValue(chunks, memoryContext),
      this._generateCustomerInterests(chunks, memoryContext),
    ]);

    const customerNextSteps = await this._generateNextSteps(
      [...chunks, ...customerEcosystem, ...customerValue, ...customerInterests],
      memoryContext,
    );

    return {
      accountNumber: accountData.accountNumber,
      generatedAt: new Date(),
      categories: {
        customer_ecosystem: customerEcosystem,
        customer_value: customerValue,
        customer_interests: customerInterests,
        next_steps: customerNextSteps,
      },
    };
  }

  private async _fetchChunks(
    accountNumber: string,
    query: string,
  ): Promise<{
    chunks: string[];
    stats: { chunkCount: number; tokenCount: number };
  }> {
    const accountSummaryChunks = await this.vectorSearch.fetchRelevantChunks({
      accountNumber,
      query,
      limit: 5,
      sourceType: "account-summary",
    });

    const solutionsChunks = await this.vectorSearch.fetchRelevantChunks({
      query,
      limit: 10,
      scope: "sales-assets",
    });

    const allChunks = [...accountSummaryChunks, ...solutionsChunks];

    const selectedChunks: string[] = [];
    let runningTotal = 0;

    for (const chunk of allChunks) {
      const tokens = estimateTokenCount(chunk);
      if (runningTotal + tokens > config.insightLLM.maxPromptTokens) break;

      selectedChunks.push(chunk);
      runningTotal += tokens;
    }

    return {
      chunks: selectedChunks,
      stats: {
        chunkCount: selectedChunks.length,
        tokenCount: runningTotal,
      },
    };
  }

  private async _generateCustomerEcosystem(
    context: string[],
    memoryContext?: string,
  ): Promise<string[]> {
    const messages = buildCustomerEcosystemPrompt(context, memoryContext);
    const response = await this.llm.generate({ messages });
    this.logger.log(`Customer Ecosystem: ${response.content}`);
    const extracted = extractSingleInsightSection(response.content, "customer_ecosystem");
    return extracted;
  }

  private async _generateCustomerValue(
    context: string[],
    memoryContext?: string,
  ): Promise<string[]> {
    const messages = buildCustomerValuePrompt(context, memoryContext);
    const response = await this.llm.generate({ messages });
    this.logger.log(`Customer Value: ${response.content}`);
    const extracted = extractSingleInsightSection(response.content, "customer_value");
    return extracted;
  }

  private async _generateCustomerInterests(
    context: string[],
    memoryContext?: string,
  ): Promise<string[]> {
    const messages = buildCustomerInterestsPrompt(context, memoryContext);
    const response = await this.llm.generate({ messages });
    this.logger.log(`Customer Interests: ${response.content}`);
    const extracted = extractSingleInsightSection(response.content, "customer_interests");
    return extracted;
  }

  private async _generateNextSteps(context: string[], memoryContext?: string): Promise<string[]> {
    const messages = buildNextStepsPrompt(context, memoryContext);
    const response = await this.llm.generate({ messages });
    this.logger.log(`Next Steps: ${response.content}`);
    const extracted = extractSingleInsightSection(response.content, "next_steps");
    return extracted;
  }
}
```

### Chunking Use Cases

```typescript
// From .references/c1-rsf-app/src/application/use-cases/chunk/account-summary-chunk.usecase.ts
import * as sbd from "sbd";

import { TAccountSummaryNormalizedData } from "@/application/dto/account.dto";
import { QdrantVectorStore } from "@/infrastructure/vector/vector-store.qdrant.service";
import { config } from "@/shared/config";
import { generateContentHash } from "@/shared/utils/hash-utils";
import { estimateTokenCount } from "@/shared/utils/token-utils";
import { BaseChunkBuilder } from "./base-chunk-builder";

export class AccountSummaryChunkUseCase extends BaseChunkBuilder {
  constructor(private readonly vectorStore = new QdrantVectorStore()) {
    super();
  }

  async run(accountData: TAccountSummaryNormalizedData): Promise<string[]> {
    const chunks = await this._chunkText(accountData.normalized);

    for (const [index, chunk] of chunks.entries()) {
      const contentHash = generateContentHash(chunk);
      const embedding = await this._safeGenerateEmbedding(chunk);

      if (!embedding.length) continue;

      await this.vectorStore.upsert({
        collectionName: config.qdrant.collection,
        id: this._generateChunkId(),
        embedding,
        payload: {
          content: chunk,
          contentHash,
          accountNumber: accountData.accountNumber,
          chunkIndex: index,
          gemStatus: accountData.gemStatus,
          topVendors: accountData.topVendors?.map((v) => v.name) ?? [],
          tokenCount: estimateTokenCount(chunk),
          sourceType: "account-summary",
          createdAt: new Date().toISOString(),
          embeddingModel: config.embeddingLLM.model,
          originalText: accountData.originalText,
        },
      });
    }

    return chunks;
  }

  protected override async _chunkText(text: string): Promise<string[]> {
    const maxTokens = config.insightLLM.maxTokensPerChunk;
    const overlapTokens = config.insightLLM.tokenOverlapPerChunk;

    const sentences = this._splitIntoSentences(text);
    const chunks: string[] = [];
    let current: string[] = [];

    for (let i = 0; i < sentences.length; i++) {
      current.push(sentences[i]);
      const joined = current.join(" ");
      const tokenCount = estimateTokenCount(joined);

      if (tokenCount > maxTokens) {
        const chunk = current.slice(0, -1).join(" ");
        if (chunk) chunks.push(chunk);

        // slice enough sentences to approximate overlapTokens
        let overlapStart = current.length - 1;
        let overlapTokenCount = 0;
        while (overlapStart > 0 && overlapTokenCount < overlapTokens) {
          overlapTokenCount += estimateTokenCount(current[overlapStart]);
          overlapStart--;
        }

        current = current.slice(overlapStart);
      }
    }

    if (current.length > 0) {
      chunks.push(current.join(" "));
    }

    return chunks;
  }

  private _splitIntoSentences(text: string): string[] {
    return sbd.sentences(text, {
      newline_boundaries: true,
      html_boundaries: true,
      sanitize: true,
      allowed_tags: false,
      abbreviations: ["e.g", "i.e", "U.S", "U.K", "Mr", "Mrs", "Dr"],
    });
  }
}
```

---

## 2.5 RSF Data Processing Flows

This section illustrates how data flows through the RSF application from import to insight generation.

### Overall System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           RSF Data Processing Pipeline                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  INPUT SOURCES           IMPORT LAYER            STORAGE LAYER              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ CSV Filesâ”‚ â”€â”€â”€â”€â”€â”€â†’  â”‚ Import       â”‚ â”€â”€â”€â”€â†’ â”‚  PostgreSQL  â”‚            â”‚
â”‚  â”‚  - Accounts         â”‚ Use Cases    â”‚       â”‚  Database    â”‚            â”‚
â”‚  â”‚  - Products         â”‚              â”‚       â”‚              â”‚            â”‚
â”‚  â”‚  - Opportunities    â”‚              â”‚       â”‚              â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                       â†“                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚Doc Files â”‚ â”€â”€â”€â”€â”€â”€â†’  â”‚ File Parser  â”‚ â”€â”€â”€â”€â†’ â”‚   Qdrant     â”‚            â”‚
â”‚  â”‚ - PPTX              â”‚ Service      â”‚       â”‚Vector Store  â”‚            â”‚
â”‚  â”‚ - PDF               â”‚              â”‚       â”‚              â”‚            â”‚
â”‚  â”‚ - DOCX              â”‚              â”‚       â”‚              â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                             â”‚
â”‚  PROCESSING LAYER                              OUTPUT LAYER                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚Normalization â”‚ â†â†’  â”‚    LLM       â”‚ â”€â”€â”€â”€â†’ â”‚   Insights   â”‚            â”‚
â”‚  â”‚  Services    â”‚     â”‚  Services    â”‚       â”‚   Export     â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Import Flows

#### 1. Account Import Flow

```
CLI Command: npm run import:accounts
â”‚
â”œâ†’ import-accounts.ts
â”‚  â””â†’ ImportAccountsUseCase
â”‚     â”œâ†’ AccountLoaderService.loadMergedAccounts()
â”‚     â”‚  â”œâ†’ Load GEM accounts CSV
â”‚     â”‚  â”œâ†’ Load CRM accounts CSV
â”‚     â”‚  â””â†’ Merge data by account number
â”‚     â”‚
â”‚     â””â†’ For each account:
â”‚        â””â†’ prisma.account.upsert()
â”‚           â”œâ†’ Create if new
â”‚           â””â†’ Update if exists
â”‚
Output: Accounts stored in PostgreSQL
```

#### 2. Product Import Flow

```
CLI Command: npm run import:products
â”‚
â”œâ†’ import-products.ts
â”‚  â””â†’ ImportProductsUseCase
â”‚     â”œâ†’ ProductLoaderService.loadAll()
â”‚     â”‚  â””â†’ Parse products CSV (10k+ rows)
â”‚     â”‚
â”‚     â””â†’ Batch processing (10,000 per batch):
â”‚        â””â†’ prisma.product.createMany()
â”‚           â””â†’ skipDuplicates: true
â”‚
Output: Products stored in PostgreSQL
```

#### 3. Asset Import Flow

```
CLI Command: npm run import:assets
â”‚
â”œâ†’ import-assets.ts
â”‚  â””â†’ ImportAssetsUseCase
â”‚     â”œâ†’ AssetLoaderService.load()
â”‚     â”‚  â””â†’ Parse asset metadata Excel file
â”‚     â”‚
â”‚     â””â†’ For each asset file:
â”‚        â”œâ†’ FileParserService.parse()
â”‚        â”‚  â”œâ†’ PPTX â†’ slides
â”‚        â”‚  â”œâ†’ PDF â†’ paragraphs
â”‚        â”‚  â”œâ†’ DOCX â†’ sections
â”‚        â”‚  â””â†’ XLSX â†’ rows
â”‚        â”‚
â”‚        â”œâ†’ DocumentNormalizerService.normalize()
â”‚        â”‚  â””â†’ Convert to standardized chunks
â”‚        â”‚
â”‚        â”œâ†’ RecursiveCharacterTextSplitter.split()
â”‚        â”‚  â””â†’ Split by token limits (400/200)
â”‚        â”‚
â”‚        â””â†’ GlobalVectorChunkBuilder.run()
â”‚           â”œâ†’ Generate embeddings (Ollama)
â”‚           â””â†’ Store in Qdrant vector DB
â”‚
Output: Document chunks in vector store
```

#### 4. Opportunity Import Flow

```
CLI Command: npm run import:opportunities
â”‚
â”œâ†’ import-opportunities.ts
â”‚  â””â†’ ImportOpportunitiesUseCase
â”‚     â”œâ†’ Load all accounts & products
â”‚     â”‚
â”‚     â”œâ†’ OpportunityLoaderService.loadPurchases()
â”‚     â”‚  â”œâ†’ Parse opportunities CSV
â”‚     â”‚  â””â†’ Parse purchase items CSV
â”‚     â”‚
â”‚     â”œâ†’ For each opportunity:
â”‚     â”‚  â””â†’ prisma.opportunity.upsert()
â”‚     â”‚
â”‚     â””â†’ For each purchase item:
â”‚        â””â†’ prisma.purchaseProduct.create()
â”‚           â””â†’ Link opportunity â†” product
â”‚
Output: Opportunities & purchase links in PostgreSQL
```

### Processing Flows

#### 5. Account Summary Normalization Flow

```
CLI Command: npm run chunk:account-summaries
â”‚
â”œâ†’ chunk-account-summaries.ts
â”‚  â””â†’ For each account:
â”‚     â”œâ†’ AccountDataService.getAccountInsightData()
â”‚     â”‚  â”œâ†’ Aggregate opportunities
â”‚     â”‚  â”œâ†’ Calculate revenue metrics
â”‚     â”‚  â”œâ†’ Extract top vendors/products
â”‚     â”‚  â””â†’ Generate account summary
â”‚     â”‚
â”‚     â”œâ†’ AccountSummaryNormalizerService.normalize()
â”‚     â”‚  â”œâ†’ Format data for LLM
â”‚     â”‚  â””â†’ Use NormalizationLLMService
â”‚     â”‚     â””â†’ Temperature: 0.4 (consistent)
â”‚     â”‚
â”‚     â””â†’ AccountSummaryChunkUseCase.run()
â”‚        â”œâ†’ Split into chunks (sbd library)
â”‚        â”œâ†’ Generate embeddings
â”‚        â””â†’ Store in Qdrant with metadata:
â”‚           â”œâ†’ accountNumber
â”‚           â”œâ†’ gemStatus
â”‚           â”œâ†’ topVendors[]
â”‚           â””â†’ sourceType: "account-summary"
â”‚
Output: Normalized account chunks in vector store
```

#### 6. Document Chunking & Vectorization Flow

```
Generic document processing pipeline:
â”‚
â”œâ†’ Parse document â†’ TParsedBlock[]
â”‚  â”œâ†’ title
â”‚  â”œâ†’ bullets[]
â”‚  â””â†’ slideNumber/sectionNumber
â”‚
â”œâ†’ Normalize content â†’ TNormalizedChunk[]
â”‚  â”œâ†’ content
â”‚  â”œâ†’ originalText
â”‚  â”œâ†’ tokenCount
â”‚  â””â†’ metadata
â”‚
â”œâ†’ Generate embeddings
â”‚  â””â†’ OllamaEmbeddingService
â”‚     â””â†’ /v1/embeddings API
â”‚
â””â†’ Store in Qdrant
   â””â†’ QdrantVectorStore.upsert()
      â”œâ†’ id (UUID)
      â”œâ†’ vector (embedding)
      â””â†’ payload (metadata)
```

### Insight Generation Flow

#### 7. Complete Insight Generation Pipeline

```
CLI Command: npm run generate:insights
â”‚
â”œâ†’ generate-insight-batch.ts
â”‚  â””â†’ For each account:
â”‚     â””â†’ GenerateInsightUseCase.run()
â”‚        â”‚
â”‚        â”œâ†’ [1] Prepare Context
â”‚        â”‚  â”œâ†’ Load account data
â”‚        â”‚  â””â†’ Combine: originalText + normalizedSummary
â”‚        â”‚
â”‚        â”œâ†’ [2] Vector Search
â”‚        â”‚  â””â†’ QdrantVectorSearchService.fetchRelevantChunks()
â”‚        â”‚     â”œâ†’ Account chunks (limit: 5)
â”‚        â”‚     â”‚  â””â†’ Filter: accountNumber match
â”‚        â”‚     â””â†’ Solution chunks (limit: 10)
â”‚        â”‚        â””â†’ Filter: scope = "sales-assets"
â”‚        â”‚
â”‚        â”œâ†’ [3] Parallel Insight Generation
â”‚        â”‚  â”œâ†’ buildCustomerEcosystemPrompt() â†’ InsightLLMService
â”‚        â”‚  â”œâ†’ buildCustomerValuePrompt() â†’ InsightLLMService
â”‚        â”‚  â”œâ†’ buildCustomerInterestsPrompt() â†’ InsightLLMService
â”‚        â”‚  â””â†’ All use temperature: 0.7
â”‚        â”‚
â”‚        â”œâ†’ [4] Sequential Next Steps
â”‚        â”‚  â””â†’ buildNextStepsPrompt()
â”‚        â”‚     â””â†’ Input: all previous insights + chunks
â”‚        â”‚
â”‚        â””â†’ [5] Format Output
â”‚           â””â†’ InsightResultDTO
â”‚              â”œâ†’ accountNumber
â”‚              â”œâ†’ generatedAt
â”‚              â””â†’ categories:
â”‚                 â”œâ†’ customer_ecosystem[]
â”‚                 â”œâ†’ customer_value[]
â”‚                 â”œâ†’ customer_interests[]
â”‚                 â””â†’ next_steps[]
â”‚
Output: JSON insights file per account
```

### Export/Query Flows

#### 8. Vector Search Flow

```
Query Processing:
â”‚
â”œâ†’ Input query text
â”‚  â””â†’ OllamaEmbeddingService.generateEmbedding()
â”‚
â”œâ†’ QdrantClient.search()
â”‚  â”œâ†’ Vector similarity search
â”‚  â”œâ†’ Apply filters (accountNumber, scope, etc.)
â”‚  â””â†’ Return top K results
â”‚
â””â†’ Extract content from payloads
   â””â†’ Return relevant text chunks
```

#### 9. Evaluation & Benchmarking Flow

```
CLI Command: npm run evaluate:query-benchmarks
â”‚
â”œâ†’ Load benchmark queries
â”‚  â””â†’ TEvaluationQueryBenchmark[]
â”‚
â”œâ†’ For each query:
â”‚  â”œâ†’ Execute vector search
â”‚  â”œâ†’ Compare with expected results
â”‚  â””â†’ Calculate metrics:
â”‚     â”œâ†’ Recall@K
â”‚     â”œâ†’ MRR (Mean Reciprocal Rank)
â”‚     â””â†’ Precision scores
â”‚
â””â†’ Generate evaluation report
```

### Data Dependencies & Sequencing

```
Import Order (Required):
1. Products â†’ (independent)
2. Accounts â†’ (independent)
3. Opportunities â†’ (requires Products & Accounts)
4. Assets â†’ (independent)

Processing Order:
1. Import all data
2. Chunk account summaries
3. Chunk opportunity details (optional)
4. Import & chunk assets
5. Generate insights

Key Services:
- FileParserService: Multi-format document parsing
- NormalizationLLMService: Consistent formatting (temp: 0.4)
- InsightLLMService: Creative insights (temp: 0.7)
- QdrantVectorStore: Embedding storage & retrieval
- AccountDataService: Complex aggregations & metrics
```

---

## 3. LLM & Vector Integration

### Flowise API Client (from E4E)

```typescript
// From .references/c1-e4e-app/src/lib/flowise-api.ts
"use server";

import type { AssistantStyle, AssistantTone, AssistantType, User } from "@/lib/prisma";
import * as Sentry from "@sentry/nextjs";
import { FlowiseClient } from "flowise-sdk";
import type { StreamResponse } from "flowise-sdk/dist/flowise-sdk";
import { getEnvVariable } from "./utils";
type FlowiseChatResponse = AsyncGenerator<StreamResponse, void, unknown>;

interface FlowiseChatRequest {
  chatId: string;
  user: User;
  query: string;
  preferences: { tone: AssistantTone; style: AssistantStyle };
  assistant: AssistantType;
}

const assistantFlowMap: Record<AssistantType, { baseUrl: string; flowId: string }> = {
  GENERAL: {
    baseUrl: getEnvVariable("FLOWISE_CHAT_GENERAL_BASE_URL"),
    flowId: getEnvVariable("FLOWISE_CHAT_GENERAL_FLOW_ID"),
  },
  RESEARCH: {
    baseUrl: getEnvVariable("FLOWISE_CHAT_RESEARCH_BASE_URL"),
    flowId: getEnvVariable("FLOWISE_CHAT_RESEARCH_FLOW_ID"),
  },
  REASON: {
    baseUrl: getEnvVariable("FLOWISE_CHAT_REASON_BASE_URL"),
    flowId: getEnvVariable("FLOWISE_CHAT_REASON_FLOW_ID"),
  },
  HUMAN_RESOURCES: {
    baseUrl: getEnvVariable("FLOWISE_CHAT_HR_BASE_URL"),
    flowId: getEnvVariable("FLOWISE_CHAT_HR_FLOW_ID"),
  },
  RFP: {
    baseUrl: getEnvVariable("FLOWISE_CHAT_RFP_BASE_URL"),
    flowId: getEnvVariable("FLOWISE_CHAT_RFP_FLOW_ID"),
  },
  MEETING: {
    baseUrl: getEnvVariable("FLOWISE_MEETING_CHAT_BASE_URL"),
    flowId: getEnvVariable("FLOWISE_MEETING_CHAT_FLOW_ID"),
  },
  ENHANCE_PROMPT: {
    baseUrl: getEnvVariable("FLOWISE_CHAT_ENHANCE_PROMPT_BASE_URL"),
    flowId: getEnvVariable("FLOWISE_CHAT_ENHANCE_PROMPT_FLOW_ID"),
  },
};

class FlowiseApi {
  static async getFlow({
    chatId,
    user,
    query,
    assistant,
    preferences,
  }: FlowiseChatRequest): Promise<FlowiseChatResponse> {
    try {
      const { baseUrl, flowId } = assistantFlowMap[assistant];

      const client = new FlowiseClient({ baseUrl, apiKey: getEnvVariable("FLOWISE_AUTH_TOKEN") });

      return await client.createPrediction({
        chatflowId: flowId,
        question: query,
        streaming: true,
        overrideConfig: {
          sessionId: chatId,
          vars: { flow: assistant, style: preferences.style, tone: preferences.tone },
        },
        analytics: { langFuse: { userId: user.oid } },
      });
    } catch (error) {
      Sentry.withScope((scope) => {
        scope.setExtra("error", error);
        Sentry.captureMessage("Failed to create prediction", { level: "error" });
      });

      throw new Error("Failed to create prediction");
    }
  }
}

export { FlowiseApi };
```

### LLM Services (from RSF)

```typescript
// From .references/c1-rsf-app/src/infrastructure/llm/insight-llm.service.ts
import {
  ILanguageModelService,
  TLanguageModelParams,
} from "@/application/interfaces/language-model.service.interface";
import { config } from "@/shared/config";
import { TAIMessage } from "@/types";
import { LoggerService } from "../log/logger.service";

export class InsightLLMService implements ILanguageModelService {
  private readonly baseUrl = config.insightLLM.serverUrl;
  private readonly model = config.insightLLM.model;
  private readonly logger = new LoggerService("insight", "insights.json");

  async generate(params: TLanguageModelParams): Promise<TAIMessage> {
    const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        ...(config.insightLLM.apiKey && {
          Authorization: `Bearer ${config.insightLLM.apiKey}`,
        }),
      },
      body: JSON.stringify({
        model: this.model,
        messages: params.messages,
        temperature: 0.7,
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`[Insight LLM] Failed: ${response.status} ${errorText}`);
    }
    const json = await response.json();

    await this.logger.log(JSON.stringify(json, null, 2));

    const result = json.choices?.[0]?.message?.content;

    return {
      role: "assistant",
      content: result?.trim() ?? "",
    };
  }
}

// From .references/c1-rsf-app/src/infrastructure/llm/normalization-llm.service.ts
import {
  ILanguageModelService,
  TLanguageModelParams,
} from "@/application/interfaces/language-model.service.interface";
import { config } from "@/shared/config";
import { TAIMessage } from "@/types";

export class NormalizationLLMService implements ILanguageModelService {
  private readonly baseUrl = config.normalizationLLM.serverUrl;
  private readonly model = config.normalizationLLM.model;

  async generate(params: TLanguageModelParams): Promise<TAIMessage> {
    const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        ...(config.normalizationLLM.apiKey && {
          Authorization: `Bearer ${config.normalizationLLM.apiKey}`,
        }),
      },
      body: JSON.stringify({
        model: this.model,
        messages: params.messages,
        temperature: 0.4,
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`[Normalization LLM] Failed: ${response.status} ${errorText}`);
    }

    const json = await response.json();
    const result = json.choices?.[0]?.message?.content;

    return {
      role: "assistant",
      content: result?.trim() ?? "",
    };
  }
}
```

### Vector Store Services (from RSF)

```typescript
// From .references/c1-rsf-app/src/infrastructure/vector/vector-store.qdrant.service.ts
import { QdrantClient } from "@qdrant/js-client-rest";

import { config } from "@/shared/config";

export class QdrantVectorStore {
  private readonly client = new QdrantClient({ url: config.qdrant.url });

  constructor() {
    this._initCollection(); // Optional startup safety
  }

  private async _initCollection(): Promise<void> {
    const collections = await this.client.getCollections();
    const exists = collections.collections.some((c) => c.name === config.qdrant.collection);

    if (!exists) {
      await this.client.createCollection(config.qdrant.collection, {
        vectors: {
          size: Number(config.qdrant.vectorDimensions),
          distance: config.qdrant.distanceMetric,
        },
      });

      console.log(`ğŸ†• Created Qdrant collection: ${config.qdrant.collection}`);
    }
  }

  async upsert(params: { collectionName: string; id: string; embedding: number[]; payload: any }) {
    if (!params.embedding || !Array.isArray(params.embedding) || params.embedding.length === 0) {
      throw new Error(`[QdrantVectorStore] Embedding is missing or invalid for ID: ${params.id}`);
    }

    await this.client.upsert(params.collectionName, {
      points: [
        {
          id: params.id,
          vector: params.embedding,
          payload: params.payload,
        },
      ],
    });
  }
}

// From .references/c1-rsf-app/src/infrastructure/vector/vector-search.qdrant.service.ts
import { QdrantClient } from "@qdrant/js-client-rest";

import { OllamaEmbeddingService } from "@/infrastructure/embedding/embedding.ollama.service";
import { config } from "@/shared/config";
import {
  TVectorMustFilter,
  TVectorScope,
  TVectorSearchScoredPoint,
  TVectorSourceType,
} from "@/types/vector";

export class QdrantVectorSearchService {
  private readonly client = new QdrantClient({ url: config.qdrant.url });
  private readonly embedder = new OllamaEmbeddingService();

  async getById(id: string): Promise<TVectorSearchScoredPoint | null> {
    const results = await this.client.retrieve(config.qdrant.collection, {
      with_payload: true,
      ids: [id.trim()],
      with_vector: true,
    });

    return results.map((p) => ({
      ...p,
      version: Number(p.order_value ?? 0),
      score: Number(p.order_value ?? 0),
    }))[0];
  }

  async getAllDocuments(params: { scope: string }): Promise<TVectorSearchScoredPoint[]> {
    const results = await this.client.scroll(config.qdrant.collection, {
      filter: {
        must: [{ key: "scope", match: { value: params.scope } }],
      },
      with_payload: true,
      with_vector: false,
    });

    return results.points.map((p) => ({
      ...p,
      version: Number(p.order_value ?? 0),
      score: Number(p.order_value ?? 0),
    }));
  }

  async fetchRelevantChunks(params: {
    accountNumber?: string;
    query: string;
    scope?: TVectorScope;
    sourceType?: TVectorSourceType;
    accessType?: string;
    limit?: number;
  }): Promise<string[]> {
    const { accountNumber, query, scope, sourceType, accessType, limit = 5 } = params;
    const embedding = await this.embedder.generateEmbedding(query);

    const mustFilters: TVectorMustFilter = [];

    if (accountNumber) {
      mustFilters.push({ key: "accountNumber", match: { value: accountNumber } });
    }

    if (scope) {
      mustFilters.push({ key: "scope", match: { value: scope } });
    }

    if (sourceType) {
      mustFilters.push({ key: "sourceType", match: { value: sourceType } });
    }

    if (accessType) {
      mustFilters.push({ key: "accessType", match: { value: accessType } });
    }

    const results = await this.client.search(config.qdrant.collection, {
      vector: embedding,
      limit,
      filter: {
        must: mustFilters,
      },
    });

    return results.map((res) => String(res.payload?.content ?? ""));
  }

  async searchWithEmbedding(params: {
    embedding: number[];
    topK: number;
    scope: string;
  }): Promise<TVectorSearchScoredPoint[]> {
    const results = await this.client.search(config.qdrant.collection, {
      vector: params.embedding,
      limit: params.topK,
      filter: {
        must: [{ key: "scope", match: { value: params.scope } }],
      },
      with_payload: true,
      with_vector: false,
    });

    return results.map((res) => ({
      ...res,
      version: Number(res.order_value ?? 0),
      score: Number(res.score ?? 0),
    }));
  }
}
```

### Embedding Service (from RSF)

```typescript
// From .references/c1-rsf-app/src/infrastructure/embedding/embedding.ollama.service.ts
import { IEmbeddingService } from "@/application/interfaces/embedding.service.interface";
import { config } from "@/shared/config";

export class OllamaEmbeddingService implements IEmbeddingService {
  private readonly baseUrl = config.embeddingLLM.serverUrl;
  private readonly model = config.embeddingLLM.model;

  async generateEmbedding(input: string): Promise<number[]> {
    const response = await fetch(`${this.baseUrl}/v1/embeddings`, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        ...(config.embeddingLLM.apiKey && {
          Authorization: `Bearer ${config.embeddingLLM.apiKey}`,
        }),
      },
      body: JSON.stringify({
        model: this.model,
        input,
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`[Embedding] Failed to fetch: ${response.status} ${errorText}`);
    }

    const json = await response.json();
    const embedding = json?.data?.[0]?.embedding;

    if (!embedding || !Array.isArray(embedding)) {
      throw new Error(`[Embedding] Invalid or missing embedding: ${JSON.stringify(json)}`);
    }

    return embedding;
  }
}
```

---

## 4. File Processing & Import

### File Parser Service (from RSF)

```typescript
// From .references/c1-rsf-app/src/infrastructure/import/file-parser.service.ts
import fs from "node:fs";
import path from "node:path";

import { TParsedBlock } from "@/types";
import { CsvParser } from "./parsers/csv.parser";
import { DocParser } from "./parsers/doc.parser";
import { ExcelParser } from "./parsers/excel.parser";
import { PdfParser } from "./parsers/pdf.parser";
import { PptxParser } from "./parsers/pptx.parser";

export class FileParserService {
  private readonly csvParser = new CsvParser();
  private readonly excelParser = new ExcelParser();
  private readonly pdfParser = new PdfParser();
  private readonly docParser = new DocParser();
  private readonly pptxParser = new PptxParser();

  async listFilePaths(dir: string): Promise<string[]> {
    return fs.promises.readdir(dir).then((files) => files.map((file) => path.resolve(dir, file)));
  }

  async parseCsv<Row extends Record<string, string>>(filePath: string): Promise<Row[]> {
    return this.csvParser.parse<Row>(filePath);
  }

  async parseExcel<Row extends Record<string, unknown>>(filePath: string): Promise<Row[]> {
    return this.excelParser.parse<Row>(filePath);
  }

  async parse(filePath: string): Promise<TParsedBlock[]> {
    const ext = path.extname(filePath).toLowerCase();

    if (ext.includes("pptx")) {
      const slides = await this.pptxParser.parse(filePath);
      return slides.map((slide) => ({
        title: slide.title,
        bullets: slide.bullets,
        slideNumber: slide.slideNumber,
      }));
    }

    if (ext.includes("pdf")) {
      const { text } = await this.pdfParser.parse(filePath);
      return text.split("\n\n").map((para, i) => ({
        title: `Paragraph ${i + 1}`,
        bullets: [para.trim()],
        sectionNumber: i + 1,
        rawText: para.trim(),
      }));
    }

    // Additional parsers for DOCX, XLSX...
    throw new Error(`Unsupported file type: ${ext}`);
  }
}
```

### CSV Parser (from RSF)

```typescript
// From .references/c1-rsf-app/src/infrastructure/import/parsers/csv.parser.ts
import { parse as csvParse } from "csv-parse/sync";
import fs from "node:fs";

import { IFileParserService } from "@/application/interfaces/file-parser.service.interface";

export class CsvParser implements IFileParserService<Record<string, string>[]> {
  supports(ext: string): boolean {
    return ext === ".csv";
  }

  async parse<T = Record<string, string>>(filePath: string): Promise<T[]> {
    const content = fs.readFileSync(filePath, "utf-8");
    return csvParse(content, { columns: true, skip_empty_lines: true }) as T[];
  }
}
```

---

## 5. Prompt Engineering (from RSF)

### Customer Ecosystem Prompt (from RSF)

```typescript
// From .references/c1-rsf-app/src/shared/prompts/insights/customer-ecosystem.prompt.ts
import {
  plainTextBulletScaffold,
  plainTextCSVInstructions,
  plainTextOutputAnchor,
  plainTextOutputHeader,
} from "./shared-output-structure";

export const customerEcosystemPrompt = `
You are a strategic business consultant analyzing customer data. Your task is to generate high-impact, context-grounded insights focused on the customer's technology ecosystem and vendor landscape.

ACCOUNT DATA:
{{context}}

CONTEXT INFORMATION:
{{memory_context}}

${plainTextCSVInstructions}

Only propose relevant vendors, platforms, or technologies if:
- They align with the customer's evaluated solutions, strategic initiatives, or known technology needs.
- They are commonly used in similar architectures for this customer's industry or problem domain.
- They appear in the provided context.

Do not include vendors or technologies that are unrelated to the customer's stated or implied direction.

Generate exactly four bullet points that:
- Identify key platforms and technologies already in use.
- Recommend upgrades, integrations, or shifts based on gaps.
- Highlight strategic vendor relationships worth preserving or expanding.
- Reference recent sales activity, top vendors, and key purchased products when helpful.

${plainTextOutputHeader}
${plainTextOutputAnchor("CUSTOMER_ECOSYSTEM")}

${plainTextBulletScaffold}
`.trim();
```

### Prompt Builder (from RSF)

```typescript
// From .references/c1-rsf-app/src/shared/prompts/insights/insight-prompt-builder.ts
import { TAIMessage } from "@/types";
import { customerEcosystemPrompt } from "./customer-ecosystem.prompt";
import { customerInterestsPrompt } from "./customer-interests.prompt";
import { customerValuePrompt } from "./customer-value.prompt";
import { nextStepsPrompt } from "./next-steps.prompt";

function interpolatePrompt(template: string, context: string[], memoryContext?: string): string {
  return template
    .replace("{{context}}", context.join("\n\n"))
    .replace("{{memory_context}}", memoryContext || "None");
}

export function buildCustomerEcosystemPrompt(
  context: string[],
  memoryContext?: string,
): TAIMessage[] {
  const prompt = interpolatePrompt(customerEcosystemPrompt, context, memoryContext);
  return [
    { role: "system", content: "You are a strategic consultant providing ecosystem insights." },
    { role: "user", content: prompt },
  ];
}

export function buildCustomerValuePrompt(context: string[], memoryContext?: string): TAIMessage[] {
  const prompt = interpolatePrompt(customerValuePrompt, context, memoryContext);
  return [
    {
      role: "system",
      content:
        "You are a strategic consultant identifying growth opportunities and strategic value.",
    },
    { role: "user", content: prompt },
  ];
}

export function buildCustomerInterestsPrompt(
  context: string[],
  memoryContext?: string,
): TAIMessage[] {
  const prompt = interpolatePrompt(customerInterestsPrompt, context, memoryContext);
  return [
    {
      role: "system",
      content:
        "You are a strategic consultant identifying customer interests, risks, and engagement gaps.",
    },
    { role: "user", content: prompt },
  ];
}

export function buildNextStepsPrompt(context: string[], memoryContext?: string): TAIMessage[] {
  const prompt = interpolatePrompt(nextStepsPrompt, context, memoryContext);
  return [
    {
      role: "system",
      content:
        "You are a strategic consultant recommending immediate next actions for the account team.",
    },
    { role: "user", content: prompt },
  ];
}
```

---

## 6. Configuration & Environment

### RSF Configuration

```typescript
// From .references/c1-rsf-app/src/shared/config.ts
import dotenv from "dotenv";
dotenv.config();

function requireEnv(key: string): string {
  const value = process.env[key];
  if (!value) throw new Error(`Missing environment variable: ${key}`);
  return value;
}

function parseIntEnv(key: string): number {
  return parseInt(requireEnv(key), 10);
}

type TQdrantDistanceMetric = "Cosine" | "Euclid" | "Dot" | "Manhattan";

export const config = {
  utilityLLM: {
    serverUrl: requireEnv("LLM_UTILITY_SERVER_URL"),
    apiKey: requireEnv("LLM_UTILITY_API_KEY"),
    model: requireEnv("LLM_UTILITY_MODEL"),
    tokenLimit: parseIntEnv("LLM_UTILITY_TOKEN_LIMIT"),
    reserveForResponse: parseIntEnv("LLM_UTILITY_TOKEN_RESERVE_FOR_RESPONSE"),
    get maxPromptTokens() {
      return this.tokenLimit - this.reserveForResponse;
    },
  },
  insightLLM: {
    serverUrl: requireEnv("LLM_INSIGHT_SERVER_URL"),
    apiKey: requireEnv("LLM_INSIGHT_API_KEY"),
    model: requireEnv("LLM_INSIGHT_MODEL"),
    tokenLimit: parseIntEnv("LLM_INSIGHT_TOKEN_LIMIT"),
    maxTokensPerChunk: parseIntEnv("LLM_INSIGHT_MAX_TOKENS_PER_CHUNK"),
    tokenOverlapPerChunk: parseIntEnv("LLM_INSIGHT_TOKEN_OVERLAP_PER_CHUNK"),
  },
  embeddingLLM: {
    serverUrl: requireEnv("LLM_EMBEDDING_SERVER_URL"),
    apiKey: requireEnv("LLM_EMBEDDING_API_KEY"),
    model: requireEnv("LLM_EMBEDDING_MODEL"),
  },
  qdrant: {
    url: requireEnv("QDRANT_URL"),
    collection: requireEnv("QDRANT_COLLECTION"),
    vectorDimensions: parseIntEnv("QDRANT_VECTOR_DIMENSIONS"),
    distanceMetric: requireEnv("QDRANT_DISTANCE_METRIC") as TQdrantDistanceMetric,
    defaultTopK: parseIntEnv("QDRANT_DEFAULT_TOP_K"),
  },
};
```

### Dependencies Comparison

```json
// RSF Dependencies (Processing & AI)
{
  "@prisma/client": "^6.8.2",
  "@qdrant/js-client-rest": "^1.14.0",
  "csv-parse": "^5.6.0",
  "csv-writer": "^1.6.0",
  "gpt-tokenizer": "^2.9.0",
  "mammoth": "^1.9.0",
  "pdf-parse": "^1.1.1",
  "pptx2json": "^0.0.10",
  "sbd": "^1.0.19",
  "xlsx": "^0.18.5",
  "ora": "^8.2.0",
  "chalk": "^5.4.1"
}

// E4E Dependencies (UI & Streaming)
{
  "flowise-sdk": "^1.0.9",
  "@tanstack/react-query": "^5.66.0",
  "framer-motion": "^12.0.11",
  "sonner": "^1.7.4",
  "zustand": "^5.0.3",
  "react-markdown": "^9.0.3",
  "remark-gfm": "^4.0.0"
}

// Current Northstar Dependencies
{
  "@prisma/client": "^6.6.0",
  "@tanstack/react-query": "^5.66.0",
  "next-auth": "^5.0.0-beta.25",
  "zustand": "^5.0.3",
  "react-markdown": "^9.0.3"
}
```

---

## 7. UI Patterns & Components (from E4E)

### Streaming Hook (from E4E)

```typescript
// From .references/c1-e4e-app/src/features/assistants/hooks/use-general-ai-reply-stream.tsx
"use client";

import { streamAIResponseAction } from "@/features";
import type { ChatWithQueries, TUserQuery } from "@/lib/prisma";
import { useQueryClient } from "@tanstack/react-query";

type TStreamAIReplyParams = {
  chatId: string;
  query: TUserQuery;
};

type TStreamAIReplyResponse = {
  streamReply: (params: TStreamAIReplyParams) => Promise<string>;
};

export const useGeneralAIReplyStream = (): TStreamAIReplyResponse => {
  const queryClient = useQueryClient();

  const streamReply = async ({ chatId, query }: TStreamAIReplyParams): Promise<string> => {
    const stream = await streamAIResponseAction({ chatId, query: query.content });
    const reader = stream.getReader();
    let accumulatedResponse = "";

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;
      accumulatedResponse += value;

      queryClient.setQueryData<ChatWithQueries | undefined>(["chat", chatId], (prevChat) => {
        if (!prevChat) return prevChat;
        return {
          ...prevChat,
          queries: prevChat.queries.map((q) =>
            q.id === query.id ? { ...q, aiResponse: accumulatedResponse, completed: false } : q,
          ),
        };
      });
    }
    return accumulatedResponse;
  };

  return { streamReply };
};
```

### Table Component (from E4E)

```typescript
// From .references/c1-e4e-app/src/components/ui/table.tsx
import { cn } from "@/lib/utils";
import type { HTMLAttributes, TdHTMLAttributes, ThHTMLAttributes } from "react";
import { forwardRef } from "react";

const Table = forwardRef<HTMLTableElement, HTMLAttributes<HTMLTableElement>>(
  ({ className, ...props }, ref) => (
    <div className="relative w-full overflow-auto">
      <table ref={ref} className={cn("w-full caption-bottom text-sm", className)} {...props} />
    </div>
  )
);

const TableHeader = forwardRef<HTMLTableSectionElement, HTMLAttributes<HTMLTableSectionElement>>(
  ({ className, ...props }, ref) => (
    <thead ref={ref} className={cn("[&_tr]:border-b", className)} {...props} />
  )
);

const TableBody = forwardRef<HTMLTableSectionElement, HTMLAttributes<HTMLTableSectionElement>>(
  ({ className, ...props }, ref) => (
    <tbody ref={ref} className={cn("[&_tr:last-child]:border-0", className)} {...props} />
  )
);

const TableRow = forwardRef<HTMLTableRowElement, HTMLAttributes<HTMLTableRowElement>>(
  ({ className, ...props }, ref) => (
    <tr ref={ref} className={cn("border-b transition-colors hover:bg-muted/50 data-[state=selected]:bg-muted", className)} {...props} />
  )
);

const TableHead = forwardRef<HTMLTableCellElement, ThHTMLAttributes<HTMLTableCellElement>>(
  ({ className, ...props }, ref) => (
    <th ref={ref} className={cn("h-12 px-4 text-left align-middle font-medium text-muted-foreground", className)} {...props} />
  )
);

const TableCell = forwardRef<HTMLTableCellElement, TdHTMLAttributes<HTMLTableCellElement>>(
  ({ className, ...props }, ref) => (
    <td ref={ref} className={cn("p-4 align-middle", className)} {...props} />
  )
);

export { Table, TableBody, TableCell, TableHead, TableHeader, TableRow };
```

### State Management (from E4E)

```typescript
// From .references/c1-e4e-app/src/stores/ui-store.ts
import { create } from "zustand";

interface UiStore {
  notificationCount: number;
  setNotificationCount: (notificationCount: number) => void;
}

export const useUIStore = create<UiStore>((set) => ({
  notificationCount: 0,
  setNotificationCount: (notificationCount) => set({ notificationCount }),
}));
```

---

## 8. API & Service Patterns

### Service Architecture (from RSF)

```typescript
// From .references/c1-rsf-app/src/infrastructure/persistence/account-data.prisma.service.ts
import { TAccountInsightData } from "@/application/dto/account.dto";
import { IAccountDataService } from "@/application/interfaces/account-data.service.interface";
import { prisma } from "@/lib/prisma";
import {
  TAccountOpportunitiesWithProducts,
  TPrismaClient,
  TScoredEntity,
  TUpdateAccountNormalizedSummaryParams,
} from "@/types";

export class AccountDataService implements IAccountDataService {
  private readonly db: TPrismaClient;

  constructor() {
    this.db = prisma;
  }

  async getAccountWithOpportunitiesWithProducts(params: {
    accountNumber: string;
  }): Promise<TAccountOpportunitiesWithProducts | null> {
    const { accountNumber } = params;

    const account = await this.db.account.findUnique({
      where: { accountNumber },
      include: {
        opportunities: {
          include: {
            purchaseProducts: { include: { product: true } },
          },
        },
      },
    });

    return account;
  }

  async getAccountInsightData(params: {
    accountNumber: string;
  }): Promise<TAccountInsightData | null> {
    const { accountNumber } = params;

    const account = await this.db.account.findUnique({
      where: { accountNumber },
    });

    if (!account) return null;

    const opportunities = await this.db.opportunity.findMany({
      where: { accountId: account.id },
      include: {
        purchaseProducts: { include: { product: true } },
      },
    });

    const topVendors = this._extractTopVendors(opportunities);
    const topProducts = this._extractTopProducts(opportunities);
    const totalRevenue = this._calculateRevenue(opportunities);

    return {
      accountNumber: account.accountNumber,
      industry: account.ceCustomerSegment ?? undefined,
      gemStatus: account.gemStatus ?? undefined,
      focusSolution: account.targetSolutions ?? undefined,
      opportunityCount: opportunities.length,
      pipelineValue: opportunities.reduce((sum, o) => sum + (o.pipelineGrossRevenue ?? 0), 0),
      bookedRevenue: opportunities.reduce((sum, o) => sum + (o.bookedGrossRevenue ?? 0), 0),
      // ... aggregation logic
      account,
      opportunities,
      topVendors,
      topProducts,
      totalRevenue,
    };
  }

  private _extractTopProducts(opportunities: any[]): TScoredEntity[] {
    // Complex aggregation logic for scoring products by revenue/count
    const productMap: Record<string, TScoredEntity> = {};
    // ... implementation
    return Object.values(productMap)
      .sort((a, b) => b.totalAmount - a.totalAmount)
      .slice(0, 5);
  }
}
```

### Authentication (from E4E)

```typescript
// From .references/c1-e4e-app/src/app/(auth)/auth.ts
import { prisma } from "@/lib/prisma";
import { getEnvVariable } from "@/lib/utils";
import { PrismaAdapter } from "@auth/prisma-adapter";
import type { NextAuthConfig } from "next-auth";
import NextAuth from "next-auth";
import MicrosoftEntraIdProvider from "next-auth/providers/microsoft-entra-id";

const providers = [
  MicrosoftEntraIdProvider({
    clientId: getEnvVariable("AUTH_MICROSOFT_ENTRA_ID_ID"),
    clientSecret: getEnvVariable("AUTH_MICROSOFT_ENTRA_ID_SECRET"),
    issuer: `https://login.microsoftonline.com/${getEnvVariable("AUTH_MICROSOFT_ENTRA_ID_TENANT_ID")}/v2.0`,
  }),
];

export const authOptions: NextAuthConfig = {
  providers,
  adapter: PrismaAdapter(prisma),
  session: { strategy: "jwt" },
  callbacks: {
    async jwt({ token, account }) {
      if (account) {
        token.accessToken = account.access_token;
        token.refreshToken = account.refresh_token;
        token.accessTokenExpires = Number(Date.now() + (account.expires_in ?? 0) * 1000);
      }
      return token;
    },
    async session({ session, token }) {
      session.accessToken = token.accessToken as string;
      session.user.id = token.sub as string;
      return session;
    },
  },
  events: {
    async signIn(message) {
      if (message.user.email) {
        const user = await prisma.user.findUnique({
          where: { email: message.user.email },
        });
        if (user && message.profile?.oid && !user.oid) {
          await prisma.user.update({
            where: { id: user.id },
            data: { oid: message.profile.oid },
          });
        }
      }
    },
  },
};

export const { handlers, auth, signIn, signOut } = NextAuth(authOptions);
```

---

## 9. Integration Recommendations

### Required Database Schema Changes

1. **Add Processing Models**:

   ```prisma
   model ProcessingJob {
     id          String   @id @default(uuid())
     type        JobType  // IMPORT_ACCOUNTS, GENERATE_INSIGHTS, etc.
     status      JobStatus @default(PENDING)
     progress    Int       @default(0)
     totalItems  Int       @default(0)
     errorMessage String?
     userId      String
     user        User     @relation(fields: [userId], references: [id])
     createdAt   DateTime @default(now())
     updatedAt   DateTime @updatedAt
   }

   model Document {
     id          String    @id @default(uuid())
     fileName    String
     originalName String
     filePath    String
     fileType    String    // csv, pdf, pptx, docx, etc.
     fileSize    Int
     status      DocumentStatus @default(UPLOADED)
     userId      String
     user        User      @relation(fields: [userId], references: [id])
     createdAt   DateTime  @default(now())
     processedAt DateTime?
   }

   enum JobType {
     IMPORT_ACCOUNTS
     IMPORT_OPPORTUNITIES
     IMPORT_PRODUCTS
     IMPORT_ASSETS
     GENERATE_INSIGHTS
     CHUNK_DATA
   }

   enum JobStatus {
     PENDING
     RUNNING
     COMPLETED
     FAILED
     CANCELLED
   }
   ```

2. **Extend User Model**:

   ```prisma
   model User {
     // ... existing fields
     processingJobs ProcessingJob[]
     documents     Document[]
     preferences   Json? // Store UI preferences, LLM settings
   }
   ```

3. **Add Business Data Models** (from RSF):
   - Copy Account, Opportunity, Product, Engineer models
   - Add normalized summary and insight fields
   - Add vector search metadata

### API Endpoints to Create

1. **File Processing APIs**:

   ```typescript
   // POST /api/upload - File upload with validation
   // GET /api/jobs - List processing jobs for user
   // GET /api/jobs/[id] - Get job status and progress
   // POST /api/jobs/[id]/cancel - Cancel running job

   // POST /api/import/accounts - Import accounts from uploaded CSV
   // POST /api/import/opportunities - Import opportunities
   // POST /api/import/products - Import products
   // POST /api/import/assets - Import and parse documents
   ```

2. **Insight Generation APIs**:

   ```typescript
   // POST /api/insights/generate - Generate insights for account
   // GET /api/insights/[accountId] - Get cached insights
   // POST /api/insights/batch - Batch generate insights
   ```

3. **Vector Search APIs**:
   ```typescript
   // POST /api/search/chunks - Search document chunks
   // POST /api/search/accounts - Search account data
   // GET /api/search/similar/[chunkId] - Find similar chunks
   ```

### Dependencies to Add

```json
{
  "dependencies": {
    "@qdrant/js-client-rest": "^1.14.0",
    "csv-parse": "^5.6.0",
    "csv-writer": "^1.6.0",
    "gpt-tokenizer": "^2.9.0",
    "mammoth": "^1.9.0",
    "pdf-parse": "^1.1.1",
    "pptx2json": "^0.0.10",
    "sbd": "^1.0.19",
    "xlsx": "^0.18.5",
    "multer": "^1.4.5-lts.1",
    "@types/multer": "^1.4.11",
    "socket.io": "^4.7.5",
    "socket.io-client": "^4.7.5"
  }
}
```

### Components to Build

1. **File Management Components**:
   - `FileUploadDropzone` - Drag & drop file upload
   - `FileList` - Display uploaded files with status
   - `FileTypeIcon` - Icons for different file types
   - `ProcessingProgress` - Real-time progress bars

2. **Job Management Components**:
   - `JobQueue` - Display processing jobs
   - `JobStatus` - Status badges and progress indicators
   - `JobDetails` - Detailed job information modal
   - `JobActions` - Cancel, retry, download results

3. **Data Visualization Components**:
   - `InsightCard` - Display generated insights
   - `AccountSummary` - Account overview with metrics
   - `VendorChart` - Top vendors visualization
   - `OpportunityTimeline` - Opportunity progression

4. **Configuration Components**:
   - `LLMSettings` - Configure AI model parameters
   - `VectorSearchConfig` - Vector database settings
   - `ImportMapping` - CSV column mapping interface

### Configuration Setup

```env
# LLM Configuration
LLM_INSIGHT_SERVER_URL=http://localhost:11434
LLM_INSIGHT_API_KEY=optional
LLM_INSIGHT_MODEL=llama3.1:8b
LLM_INSIGHT_TOKEN_LIMIT=8192
LLM_INSIGHT_TOKEN_RESERVE_FOR_RESPONSE=1024
LLM_INSIGHT_MAX_TOKENS_PER_CHUNK=512
LLM_INSIGHT_TOKEN_OVERLAP_PER_CHUNK=128

# Embedding Configuration
LLM_EMBEDDING_SERVER_URL=http://localhost:11434
LLM_EMBEDDING_MODEL=nomic-embed-text
LLM_EMBEDDING_TOKEN_LIMIT=2048

# Vector Database
QDRANT_URL=http://localhost:6333
QDRANT_COLLECTION=northstar_vectors
QDRANT_VECTOR_DIMENSIONS=768
QDRANT_DISTANCE_METRIC=Cosine
QDRANT_DEFAULT_TOP_K=10

# File Processing
MAX_FILE_SIZE=50000000  # 50MB
ALLOWED_FILE_TYPES=csv,pdf,docx,pptx,xlsx
UPLOAD_DIR=./uploads
PROCESSING_TEMP_DIR=./temp

# Job Processing
MAX_CONCURRENT_JOBS=3
JOB_TIMEOUT_MINUTES=30
ENABLE_BACKGROUND_PROCESSING=true
```

---

## 10. Implementation Roadmap

### Phase 1: Database & Types Setup (Week 1-2)

**1.1 Schema Migration**

- [ ] Add ProcessingJob, Document, and enum types to Prisma schema
- [ ] Extend User model with processing relationships
- [ ] Add business data models (Account, Opportunity, Product) from RSF
- [ ] Run database migration and test schema changes
- [ ] Generate updated Prisma client

**1.2 Type System**

- [ ] Create TypeScript types for all new models
- [ ] Add DTOs for API requests/responses
- [ ] Create interfaces for services (IFileParser, ILLMService, etc.)
- [ ] Add union types for job statuses and file types

**1.3 Environment Setup**

- [ ] Add LLM and vector database environment variables
- [ ] Create config service similar to RSF pattern
- [ ] Set up local Qdrant instance for development
- [ ] Configure file upload directories and permissions

### Phase 2: Core Processing Integration (Week 3-5)

**2.1 File Processing Services**

- [ ] Implement FileParserService with CSV, PDF, DOCX parsers
- [ ] Create upload handling with Multer integration
- [ ] Add file validation and virus scanning
- [ ] Implement progress tracking for large files

**2.2 LLM Integration**

- [ ] Create LLM service classes (InsightLLM, EmbeddingLLM)
- [ ] Implement prompt builders and templates
- [ ] Add token counting and context management
- [ ] Create vector embedding and chunking services

**2.3 Job Processing System**

- [ ] Implement job queue with background processing
- [ ] Create job runners for each import type
- [ ] Add job status tracking and error handling
- [ ] Implement WebSocket for real-time updates

**2.4 Vector Search Integration**

- [ ] Set up Qdrant client and connection management
- [ ] Implement vector store and search services
- [ ] Create chunk management and retrieval
- [ ] Add similarity search and filtering

### Phase 3: UI Components & API (Week 6-8)

**3.1 API Routes**

- [ ] Create file upload API with progress tracking
- [ ] Implement job management APIs (CRUD, status)
- [ ] Add import APIs for each data type
- [ ] Create insight generation APIs
- [ ] Implement search and retrieval APIs

**3.2 UI Components**

- [ ] Build FileUploadDropzone with drag & drop
- [ ] Create JobQueue component with real-time updates
- [ ] Implement data tables for imported data
- [ ] Add insight visualization components
- [ ] Create configuration panels for LLM settings

**3.3 State Management**

- [ ] Extend UI store for job management
- [ ] Add React Query for API state management
- [ ] Implement optimistic updates for job status
- [ ] Create error boundary and notification system

**3.4 Real-time Features**

- [ ] Set up Socket.IO for live job updates
- [ ] Implement progress bars and status indicators
- [ ] Add notifications for job completion/failure
- [ ] Create live search with debouncing

### Phase 4: Testing & Deployment (Week 9-10)

**4.1 Testing**

- [ ] Unit tests for all service classes
- [ ] Integration tests for API endpoints
- [ ] Component tests for file upload and job management
- [ ] End-to-end tests for complete import workflows
- [ ] Load testing for large file processing

**4.2 Performance Optimization**

- [ ] Implement file streaming for large uploads
- [ ] Add caching for frequently accessed data
- [ ] Optimize database queries with proper indexing
- [ ] Implement rate limiting for API endpoints

**4.3 Production Setup**

- [ ] Configure production LLM endpoints
- [ ] Set up production Qdrant cluster
- [ ] Implement file storage (AWS S3 or similar)
- [ ] Add monitoring and logging (Sentry integration)
- [ ] Create backup and recovery procedures

**4.4 Documentation & Training**

- [ ] Create user documentation for file import process
- [ ] Document API endpoints and usage
- [ ] Create admin guide for system configuration
- [ ] Prepare training materials for end users

### Success Metrics

- [ ] Successfully import and process 1000+ accounts
- [ ] Generate insights in under 30 seconds per account
- [ ] Handle file uploads up to 50MB without timeout
- [ ] Maintain 99.9% uptime for processing jobs
- [ ] User satisfaction score >4.5/5 for interface usability
