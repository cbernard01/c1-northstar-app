# C1 Northstar Technical Analysis

_Generated by analyzing .references/c1-rsf-app and .references/c1-e4e-app_

---

## 1. Database Schema & Data Models

### RSF Prisma Schema

```prisma
// This is your Prisma schema file,
// learn more about it in the docs: https://pris.ly/d/prisma-schema

// Looking for ways to speed up your queries, or scale easily with your serverless or edge functions?
// Try Prisma Accelerate: https://pris.ly/cli/accelerate-init

generator client {
  provider = "prisma-client-js"
}

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

model Product {
  id                       String    @id @default(uuid())
  itemNumber               String    @unique @map("item_number")
  itemDescription          String?   @map("item_description")
  itemTypeCode             Int?      @map("item_type_code")
  itemTypeDescription      String?   @map("item_type_description")
  productType              String?   @map("product_type")
  itemRevenueCategory      String?   @map("item_revenue_category")
  itemManufacturer         String?   @map("item_manufacturer")
  itemCategory             String?   @map("item_category")
  itemLineOfBusiness       String?   @map("item_line_of_business")
  itemSubcategory          String?   @map("item_subcategory")
  itemClass                String?   @map("item_class")
  portfolio                String?   @map("portfolio")
  currentCost              Float?    @map("current_cost")
  scdStartDate             DateTime? @map("scd_start_date")
  scdEndDate               DateTime? @map("scd_end_date")
  isCurrentRecordFlag      Boolean   @map("is_current_record_flag")
  offer                    String?   @map("offer")
  practice                 String?   @map("practice")
  solutionSegment          String?   @map("solution_segment")
  businessSegment          String?   @map("business_segment")
  manufacturerPractice     String?   @map("manufacturer_practice")
  manufacturerItemCategory String?   @map("manufacturer_item_category")
  growthCategory           String?   @map("growth_category")

  PurchaseProduct PurchaseProduct[]
}

model Engineer {
  id    String @id @default(uuid())
  name  String @map("employee_name")
  email String @unique @map("employee_email")

  qualifications EngineerQualification[]

  @@index([email])
}

model EngineerQualification {
  id               String  @id @default(uuid())
  vendor           String? @map("vendor")
  qualification    String? @map("qualification_names")
  expirationStatus String? @map("expiration_status")
  status           String? @map("employee_status")

  engineerId String   @map("engineer_id")
  engineer   Engineer @relation(fields: [engineerId], references: [id])

  @@index([engineerId])
}

model Account {
  id                         String        @id @default(uuid())
  gemStatus                  String?       @map("gem_status")
  accountName                String        @map("account_name")
  accountNumber              String        @unique @map("account_number")
  timelineInfo               String?       @map("timeline_info") // Remove this column from the schema later
  crmOwner                   String?       @map("crm_owner")
  cxSeatRange                String?       @map("cx_seat_range")
  targetSolutions            String?       @map("target_solutions")
  battleCardNotes            String?       @map("battle_card_notes")
  gemIndex                   String?       @map("gem_index")
  competitorResearch         String?       @map("competitor_research")
  ccIntent                   String?       @map("cc_intent")
  ccVendor                   String?       @map("cc_vendor")
  ucIntent                   String?       @map("uc_intent")
  ucVendor                   String?       @map("uc_vendor")
  dcIntent                   String?       @map("dc_intent")
  dcVendor                   String?       @map("dc_vendor")
  enIntent                   String?       @map("en_intent")
  enVendor                   String?       @map("en_vendor")
  sxIntent                   String?       @map("sx_intent")
  sxVendor                   String?       @map("sx_vendor")
  employeeId                 String?       @map("employee_id")
  customerSummit             String?       @map("customer_summit")
  ownerName                  String?       @map("owner_name")
  managerName                String?       @map("manager_name")
  newOrgSD                   String?       @map("new_org_sd")
  dso                        String?       @map("dso")
  programCategory            String?       @map("program_category")
  finalCustomerSegment       String?       @map("final_customer_segment")
  recommendedSolution        String?       @map("recommended_solution")
  ceCustomerSegment          String?       @map("ce_customer_segment")
  activeUCRegistration       String?       @map("active_uc_registration")
  ucSeats                    String?       @map("uc_seats")
  currentPrimaryUCPlatform   String?       @map("current_primary_uc_platform")
  otherUCPlatforms           String?       @map("other_uc_platforms")
  currentAvayaRelease        String?       @map("current_avaya_release")
  activeCCRegistration       String?       @map("active_cc_registration")
  ccSeats                    String?       @map("cc_seats")
  currentCCPlatform          String?       @map("current_cc_platform")
  otherCCPlatform            String?       @map("other_cc_platform")
  customerMeeting2025        String?       @map("customer_meeting_2025")
  highestRelationshipLevel   String?       @map("highest_relationship_level")
  normalizedSummary          String?       @map("normalized_summary")
  normalizedSummaryCreatedAt DateTime?     @map("normalized_summary_created_at")
  opportunities              Opportunity[]
}

model Opportunity {
  id                   String            @id @default(uuid())
  opportunityNumber    String            @unique @map("opportunity_number")
  customerName         String            @map("customer_name")
  oppStage             String?           @map("opp_stage")
  salesPerson          String?           @map("sales_person")
  salesDirector        String?           @map("sales_director")
  bookedDate           DateTime?         @map("booked_date")
  estimatedCloseDate   DateTime?         @map("estimated_close_date")
  bookedGrossRevenue   Float             @default(0.0) @map("booked_gross_revenue")
  pipelineGrossRevenue Float             @default(0.0) @map("pipeline_gross_revenue")
  margin               Float             @default(0.0) @map("margin")
  accountId            String            @map("account_id")
  Account              Account           @relation(fields: [accountId], references: [id])
  purchaseProducts     PurchaseProduct[]

  @@index([opportunityNumber])
}

model PurchaseProduct {
  id                 String      @id @default(uuid())
  gpRevenueCategory  String?     @map("gp_revenue_category")
  mappedSolutionArea String?     @map("mapped_solution_area")
  mappedSegment      String?     @map("mapped_segment")
  mappedCapability   String?     @map("mapped_capability")
  itemCategory       String?     @map("item_category")
  opportunityId      String      @map("opportunity_id")
  productId          String      @map("product_id")
  opportunity        Opportunity @relation(fields: [opportunityId], references: [id])
  product            Product     @relation(fields: [productId], references: [id])

  @@index([opportunityId])
  @@index([productId])
}

enum PlatformType {
  CONTACT_CENTER
  UNIFIED_COMMUNICATIONS
  DATA_CENTER
  ENTERPRISE_NETWORKING
  SOFTWARE
}

enum PairingType {
  PREFERRED
  SECONDARY
}

model CXPlatformOffering {
  id               String       @id @default(uuid())
  platform         String       @map("platform")
  platformType     PlatformType @map("platform_type")
  product          String       @map("product")
  features         String[]     @map("features")
  scalabilityMin   String       @default("0") @map("scalability_min")
  scalabilityMax   String       @default("0") @map("scalability_max")
  primaryTarget    String       @map("primary_target")
  c1FocusResale    Boolean      @default(false) @map("c1_focus_resale")
  c1FocusPS        Boolean      @default(false) @map("c1_focus_ps")
  c1FocusMS        Boolean      @default(false) @map("c1_focus_ms")
  c1FocusMSDetails String?      @map("c1_focus_ms_details")
  createdAt        DateTime     @default(now()) @map("created_at")
  updatedAt        DateTime     @updatedAt @map("updated_at")
  ucPairings       CXPairing[]  @relation("UCProductPairings")
  ccPairings       CXPairing[]  @relation("CCProductPairings")
}

model CXPairing {
  id          String             @id @default(uuid())
  pairingType PairingType        @map("pairing_type")
  focus       Boolean            @default(false) @map("focus")
  ucProductId String             @map("uc_product_id")
  ccProductId String             @map("cc_product_id")
  ucProduct   CXPlatformOffering @relation("UCProductPairings", fields: [ucProductId], references: [id])
  ccProduct   CXPlatformOffering @relation("CCProductPairings", fields: [ccProductId], references: [id])

  @@unique([ucProductId, ccProductId])
}
```

### Current Northstar Schema

```prisma
// This is your Prisma schema file,
// learn more about it in the docs: https://pris.ly/d/prisma-schema

generator client {
  provider = "prisma-client-js"
}

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

// NextAuth.js models
model User {
  id            String          @id @default(cuid())
  name          String?
  email         String          @unique
  emailVerified DateTime?
  image         String?
  accounts      Account[]
  sessions      Session[]
  oid           String?         @unique // Microsoft Entra ID Object ID

  createdAt     DateTime        @default(now())
  updatedAt     DateTime        @updatedAt

  @@index([email])
  @@index([oid])
}

model Account {
  userId            String
  type              String
  provider          String
  providerAccountId String
  refresh_token     String?
  access_token      String?
  expires_at        Int?
  token_type        String?
  scope             String?
  id_token          String?
  session_state     String?

  createdAt DateTime @default(now())
  updatedAt DateTime @updatedAt

  user User @relation(fields: [userId], references: [id], onDelete: Cascade)

  @@id([provider, providerAccountId])
}

model Session {
  sessionToken String   @unique
  userId       String
  expires      DateTime
  user         User     @relation(fields: [userId], references: [id], onDelete: Cascade)

  createdAt DateTime @default(now())
  updatedAt DateTime @updatedAt
}

model VerificationToken {
  identifier String
  token      String
  expires    DateTime

  @@id([identifier, token])
}
```

### Key TypeScript Types (from RSF)

```typescript
// From .references/c1-rsf-app/src/types/account.ts
import { Account, Prisma } from "@prisma/client";

export type TAccount = Account;

export type TOpportunityWithProducts = Prisma.OpportunityGetPayload<{
  include: { purchaseProducts: { include: { product: true } } };
}>;

export type TScoredEntity = {
  name: string;
  totalAmount: number;
  count: number;
};

export type TCreateAccountParams = Omit<Account, "id" | "createdAt" | "updatedAt">;

export type TAccountOpportunitiesWithProducts = Prisma.AccountGetPayload<{
  include: { opportunities: { include: { purchaseProducts: { include: { product: true } } } } };
}>;

export type TUpdateAccountNormalizedSummaryParams = {
  accountNumber: string;
  normalizedSummary: string;
};

// From .references/c1-rsf-app/src/types/asset.ts
export type TAssetRow = {
  Name: string;
  Solution?: string;
  Offering?: string;
  "Asset Type"?: string;
};

export type TParsedBlock = {
  title?: string;
  bullets?: string[]; // Paragraphs, bullet points, table rows, etc.
  slideNumber?: number; // Optional — for PPTX
  sectionNumber?: number; // Optional — for DOCX/PDF/XLSX
};

export type TNormalizedChunk = {
  content: string;
  originalText: string;
  chunkIndex: number;
  tokenCount: number;
  contentHash: string;
  sourceFileName: string;
  sourceSection?: string;
  slideNumber?: number;
  documentCategory?: string;
  fileType: string;
  scope: string;
  normalized?: boolean;
};

// From .references/c1-rsf-app/src/types/opportunity.ts
import { Opportunity } from "@prisma/client";

export type TOpportunityCreateParams = Omit<Opportunity, "id" | "createdAt" | "updatedAt">;

export type TOpportunity = Opportunity;

// From .references/c1-rsf-app/src/types/message.ts
export type TAIMessage = {
  role: "system" | "user" | "assistant";
  content: string;
};

// From .references/c1-rsf-app/src/types/vector.ts
import { Schemas } from "@qdrant/js-client-rest";

export type TVector = number[];
export type TVectorFilter = Schemas["Filter"];
export type TVectorMustFilter = TVectorFilter["must"];
export type TVectorSearchScoredPoint = Schemas["ScoredPoint"];

export type TVectorMetadataValue = string | number | boolean | null | Date | TVectorMetadataValue[];

export type TVectorMetadata = Record<string, TVectorMetadataValue>;

export type TVectorScope = "sales-assets";

export type TVectorSourceType = "account-summary" | "account-details" | "global-context";

export type TVectorPayload = {
  content: string;
  contentHash: string;
  metadata: TVectorMetadata;
};

export type TVectorCreateParams = {
  collection: string;
  vector: TVector;
  payload: TVectorPayload;
};

export type TRelevantChunk = {
  id: string;
  score: number;
};

export type TEvaluationQueryBenchmark = {
  query: string;
  k: number;
  scope: string;
  relevantChunkIds: string[];
};

export type TEvaluationResult = {
  query: string;
  k: number;
  recallAtK: number;
  mrr: number;
  retrievedIds: string[];
  relevantChunks: TRelevantChunk[];
};

export type TRankedVectorResult = {
  id: string;
  score: number;
  content: string;
  chunkId: string;
  payload: TVectorMetadata;
};
```

### DTOs (from RSF)

```typescript
// From .references/c1-rsf-app/src/application/dto/account.dto.ts
import { TAccount, TOpportunity, TScoredEntity } from "@/types";

export type TAccountInsightData = {
  accountNumber: string;
  industry?: string;
  gemStatus?: string;
  focusSolution?: string;
  opportunityCount: number;
  pipelineValue: number;
  bookedRevenue: number;
  recentWins?: string[];
  risks?: string[];

  // Raw full data
  account: TAccount;
  opportunities: TOpportunity[];
  recentOpportunities: TOpportunity[];

  // Aggregates
  topVendors: TScoredEntity[];
  topProducts: TScoredEntity[];
  totalRevenue: number;
};

export type TAccountSummaryNormalizedData = TAccountInsightData & {
  normalized: string;
  originalText: string;
};

// From .references/c1-rsf-app/src/application/dto/asset.dto.ts
export type TAssetScope = "sales-assets" | "proposals" | "training" | "other";

export type TAssetRowResult = {
  title: string;
  sourceFile: string;
  solutions: string[];
  offerings: string[];
  assetTypes: string[];
};

export type TDocumentMetadata = {
  fileName: string;
  title?: string;
  fileType: "pptx" | "pdf" | "docx" | "xlsx";
  documentCategory?: string; // e.g., "Case Study", "Data Sheet"
  solutions?: string[];
  offerings?: string[];
  assetTypes?: string[];
  audience?: string; // e.g., "External"
  industry?: string;
  program?: string;
  sourcePath?: string;
  scope: TAssetScope;
};

// From .references/c1-rsf-app/src/application/dto/insight.dto.ts
export type TInsight = {
  accountNumber: string;
  next_steps: string[];
  customer_ecosystem: string[];
  customer_value: string[];
  customer_interests: string[];
  customer_satisfaction: string[];
};

export type TExportInsight = {
  accountNumber: string;
  next_steps: [string, string, string];
  customer_interests: [string, string, string];
  customer_value: [string, string, string];
  customer_ecosystem: [string, string, string];
};

export type TInsightCategory =
  | "customer_interests"
  | "customer_ecosystem"
  | "customer_value"
  | "next_steps";

export interface InsightResultDTO {
  accountNumber: string;
  generatedAt: Date;
  categories: Record<TInsightCategory, string[]>;
}
```

---

## 2. CLI Commands & Processing Pipeline

### Available CLI Scripts

```json
{
  "scripts": {
    "start": "tsx src/presentation/cli/main.ts",
    "dev": "tsx watch src/presentation/cli/main.ts",
    "lint": "eslint . --ext .ts",
    "format": "prettier --write .",
    "generate:insights": "tsx src/presentation/cli/generate-insight-batch.ts",
    "generate:insight-single": "tsx src/presentation/cli/generate-insight-single.ts",
    "import:accounts": "tsx src/presentation/cli/import-accounts.ts",
    "import:products": "tsx src/presentation/cli/import-products.ts",
    "import:assets": "tsx src/presentation/cli/import-assets.ts",
    "import:opportunities": "tsx src/presentation/cli/import-opportunities.ts",
    "chunk:accounts": "tsx src/presentation/cli/chunk-accounts.ts",
    "chunk:account-summaries": "tsx src/presentation/cli/chunk-account-summaries.ts",
    "chunk:opportunity-details": "tsx src/presentation/cli/chunk-opportunity-details.ts",
    "chunk:assets": "tsx src/presentation/cli/chunk-assets.ts",
    "validate:assets": "tsx src/presentation/cli/validate-import-assets.ts",
    "compare:normalizations": "tsx src/presentation/cli/compare-normalizations.ts",
    "evaluate:query-benchmarks": "tsx src/presentation/cli/evaluate-query-benchmarks.ts",
    "compare:normalizations-by-chunk-id": "tsx src/presentation/cli/compare-normalizations-by-chunk-id.ts"
  }
}
```

### Import Use Cases

```typescript
// From .references/c1-rsf-app/src/application/use-cases/import/import-accounts.usecase.ts
import { AccountLoaderService } from "@/infrastructure/import/account-loader.service";
import { ProgressTracker } from "@/shared/progress-tracker";
import { TPrismaClient } from "@/types";

export class ImportAccountsUseCase {
  constructor(
    private readonly db: TPrismaClient,
    private readonly loader = new AccountLoaderService(),
  ) {}

  async run(progress?: ProgressTracker): Promise<void> {
    const map = await this.loader.loadMergedAccounts();

    let success = 0;
    let failed = 0;

    progress?.status("🔄 Starting account import...");
    progress?.increment(0); // kick off progress display

    for (const account of map.values()) {
      try {
        await this.db.account.upsert({
          where: { accountNumber: account.accountNumber },
          create: account,
          update: account,
        });

        success++;
      } catch (err) {
        console.error(`❌ Failed to import ${account.accountNumber}`, err);
        failed++;
      } finally {
        progress?.increment();
      }
    }

    progress?.succeed(`✅ Accounts imported: ${success}`);
    if (failed) console.log(`⚠️ Accounts failed: ${failed}`);
  }
}

// From .references/c1-rsf-app/src/application/use-cases/import/import-assets.usecase.ts
import fs from "node:fs/promises";
import path from "node:path";

import { TDocumentMetadata } from "@/application/dto/asset.dto";
import { GlobalVectorChunkBuilder } from "@/application/use-cases/chunk/global-vector-chunk-builder.usecase";
import { NormalizedAssetFormatter } from "@/infrastructure/formatters/normalized-asset.formatter";
import { AssetLoaderService } from "@/infrastructure/import/asset-loader.service";
import { FileParserService } from "@/infrastructure/import/file-parser.service";
import { DocumentNormalizerService } from "@/infrastructure/normalizers/document-normalizer.service";
import { RecursiveCharacterTextSplitter } from "@/shared/recursive-splitter";
import { TVectorPayload } from "@/types/vector";

export class ImportAssetsUseCase {
  constructor(
    private readonly assetLoader = new AssetLoaderService(),
    private readonly documentNormalizer = new DocumentNormalizerService(),
    private readonly fileParser = new FileParserService(),
    private readonly chunkBuilder = new GlobalVectorChunkBuilder(),
    private readonly formatter = new NormalizedAssetFormatter(),
    private readonly splitter = new RecursiveCharacterTextSplitter(400, 200),
  ) {}

  async run(params: { assetFilePath: string; assetFolderPath: string }): Promise<void> {
    const { assetFilePath, assetFolderPath } = params;

    const rows = await this.assetLoader.load(assetFilePath);

    for (const row of rows) {
      const assetPath = path.resolve(assetFolderPath, row.sourceFile);
      const fileExt = path.extname(assetPath);
      if (fileExt === ".xlsx" || fileExt === ".xls") {
        console.warn(
          `[ImportAssetsUseCase] Skipping ${row.sourceFile}: Excel files are not supported`,
        );
        continue;
      }

      try {
        await fs.access(assetPath);

        const rawContent = await this.fileParser.parse(assetPath);

        const metadata: TDocumentMetadata = {
          fileName: row.sourceFile,
          fileType: this.detectFileType(row.sourceFile),
          title: row.title,
          solutions: row.solutions,
          offerings: row.offerings,
          assetTypes: row.assetTypes,
          scope: "sales-assets",
        };

        const normalizedChunks = await this.documentNormalizer.normalize({ rawContent, metadata });

        const formattedPayloads: TVectorPayload[] = this.formatter.format({
          chunks: normalizedChunks,
          metadata,
        });

        const chunkedPayloads = await this.splitter.splitDocuments(formattedPayloads);

        await this.chunkBuilder.run(chunkedPayloads);

        await this.logAudit({
          sourceFile: row.sourceFile,
          fileType: metadata.fileType,
          totalParsedBlocks: rawContent.length,
          totalNormalizedChunks: normalizedChunks.length,
          totalSplitChunks: chunkedPayloads.length,
          totalEmbeddedChunks: chunkedPayloads.length, // assuming all are embedded successfully
        });
      } catch (err) {
        console.warn(
          `[ImportAssetsUseCase] Skipping ${row.sourceFile}:`,
          err instanceof Error ? err.message : String(err),
        );
        // Optionally log to file
      }
    }
  }

  private detectFileType(fileName: string): TDocumentMetadata["fileType"] {
    const ext = path.extname(fileName).toLowerCase();
    if (ext.includes("ppt")) return "pptx";
    if (ext.includes("pdf")) return "pdf";
    if (ext.includes("doc")) return "docx";
    if (ext.includes("xls")) return "xlsx";
    throw new Error(`Unsupported file type: ${ext}`);
  }

  private async logAudit(params: {
    sourceFile: string;
    fileType: string;
    totalParsedBlocks: number;
    totalNormalizedChunks: number;
    totalSplitChunks: number;
    totalEmbeddedChunks: number;
  }) {
    const folder = path.resolve("data/logs/import/assets");
    await fs.mkdir(folder, { recursive: true });

    const logPath = path.join(folder, "asset-import.jsonl");

    const entry = {
      timestamp: new Date().toISOString(),
      ...params,
    };

    await fs.appendFile(logPath, JSON.stringify(entry) + "\n");
  }
}

// From .references/c1-rsf-app/src/application/use-cases/import/import-products.usecase.ts
import { ProductLoaderService } from "@/infrastructure/import/product-loader.service";
import { ProgressTracker } from "@/shared/progress-tracker";
import { TPrismaClient } from "@/types";

export class ImportProductsUseCase {
  constructor(private readonly prisma: TPrismaClient) {}

  async run(): Promise<void> {
    const loader = new ProductLoaderService();
    const products = await loader.loadAll();

    const BATCH = 10000;
    const progress = new ProgressTracker("Importing products", Math.ceil(products.length / BATCH));
    progress.start();

    let success = 0;
    let failure = 0;

    for (let i = 0; i < products.length; i += BATCH) {
      const chunk = products.slice(i, i + BATCH);
      try {
        await this.prisma.product.createMany({
          data: chunk,
          skipDuplicates: true,
        });
        success += chunk.length;
      } catch (err) {
        console.error("❌ Product batch failed:", err);
        failure += chunk.length;
      }
      progress.increment();
    }

    progress.succeed(`Products imported: ${success}, failed: ${failure}`);
  }
}

// From .references/c1-rsf-app/src/application/use-cases/import/import-opportunities.usecase.ts
import { OpportunityLoaderService } from "@/infrastructure/import/opportunity-loader.service";
import { AccountDataService } from "@/infrastructure/persistence/account-data.prisma.service";
import { ProgressTracker } from "@/shared/progress-tracker";
import { TPrismaClient } from "@/types";

export class ImportOpportunitiesUseCase {
  constructor(
    private readonly db: TPrismaClient,
    private readonly loader = new OpportunityLoaderService(),
    private readonly accountData = new AccountDataService(),
  ) {}

  async run(): Promise<void> {
    const accounts = await this.db.account.findMany();
    const products = await this.db.product.findMany();

    const { opportunities, purchaseItems } = await this.loader.loadPurchases({
      accounts,
      products,
    });

    const tracker = new ProgressTracker(
      "Importing opportunities",
      opportunities.size + purchaseItems.length,
    );
    tracker.start();

    tracker.status(`🔁 Upserted ${opportunities.size} opportunities`);

    const oppIdMap = new Map<string, string>(); // opportunityNumber → id

    for (const [oppNumber, oppData] of opportunities) {
      try {
        const result = await this.db.opportunity.upsert({
          where: { opportunityNumber: oppNumber },
          create: oppData,
          update: oppData,
        });
        oppIdMap.set(oppNumber, result.id);
      } catch (err) {
        console.error(`❌ Failed to upsert opportunity ${oppNumber}`, err);
      }
      tracker.increment();
    }

    tracker.status(`🔗 Linking ${purchaseItems.length} products to opportunities...`);

    for (const item of purchaseItems) {
      const opportunityId = oppIdMap.get(item.opportunityNumber);
      if (!opportunityId) {
        console.warn(`⚠️ Skipped purchase item — unknown opportunity: ${item.opportunityNumber}`);
        tracker.increment();
        continue;
      }

      try {
        await this.db.purchaseProduct.create({
          data: {
            opportunityId,
            productId: item.productId,
            gpRevenueCategory: item.gpRevenueCategory,
            mappedSolutionArea: item.mappedSolutionArea,
            mappedSegment: item.mappedSegment,
            mappedCapability: item.mappedCapability,
            itemCategory: item.itemCategory,
          },
        });
      } catch (err) {
        console.error(`❌ Failed to insert PurchaseProduct for ${item.opportunityNumber}`, err);
      }

      tracker.increment();
    }

    tracker.succeed("✅ Opportunity + product imports complete");
  }
}
```

### Insight Generation Use Case

```typescript
// From .references/c1-rsf-app/src/application/use-cases/insight/generate-insights.usecase.ts
import { TAccountSummaryNormalizedData } from "@/application/dto/account.dto";
import { InsightResultDTO } from "@/application/dto/insight.dto";
import { ILanguageModelService } from "@/application/interfaces/language-model.service.interface";
import { extractSingleInsightSection } from "@/infrastructure/formatters/generate-insight.formatter";
import { InsightLLMService } from "@/infrastructure/llm/insight-llm.service";
import { LoggerService } from "@/infrastructure/log/logger.service";
import { QdrantVectorSearchService } from "@/infrastructure/vector/vector-search.qdrant.service";
import { config } from "@/shared/config";
import {
  buildCustomerEcosystemPrompt,
  buildCustomerInterestsPrompt,
  buildCustomerValuePrompt,
  buildNextStepsPrompt,
} from "@/shared/prompts/insights/insight-prompt-builder";
import { estimateTokenCount } from "@/shared/utils/token-utils";

type TGenerateInsightParams = {
  accountData: TAccountSummaryNormalizedData;
  memoryContext?: string;
  debug?: boolean;
};

export class GenerateInsightUseCase {
  constructor(
    private readonly vectorSearch = new QdrantVectorSearchService(),
    private readonly llm: ILanguageModelService = new InsightLLMService(),
    private readonly logger = new LoggerService("insight", "generate-insights.json"),
  ) {}

  async run(params: TGenerateInsightParams): Promise<InsightResultDTO> {
    const { accountData, memoryContext } = params;

    const textToChunk = accountData.originalText + "\n\n" + accountData.normalized;

    const { chunks } = await this._fetchChunks(accountData.accountNumber, textToChunk);

    const [customerEcosystem, customerValue, customerInterests] = await Promise.all([
      this._generateCustomerEcosystem(chunks, memoryContext),
      this._generateCustomerValue(chunks, memoryContext),
      this._generateCustomerInterests(chunks, memoryContext),
    ]);

    const customerNextSteps = await this._generateNextSteps(
      [...chunks, ...customerEcosystem, ...customerValue, ...customerInterests],
      memoryContext,
    );

    return {
      accountNumber: accountData.accountNumber,
      generatedAt: new Date(),
      categories: {
        customer_ecosystem: customerEcosystem,
        customer_value: customerValue,
        customer_interests: customerInterests,
        next_steps: customerNextSteps,
      },
    };
  }

  private async _fetchChunks(
    accountNumber: string,
    query: string,
  ): Promise<{
    chunks: string[];
    stats: { chunkCount: number; tokenCount: number };
  }> {
    const accountSummaryChunks = await this.vectorSearch.fetchRelevantChunks({
      accountNumber,
      query,
      limit: 5,
      sourceType: "account-summary",
    });

    const solutionsChunks = await this.vectorSearch.fetchRelevantChunks({
      query,
      limit: 10,
      scope: "sales-assets",
    });

    const allChunks = [...accountSummaryChunks, ...solutionsChunks];

    const selectedChunks: string[] = [];
    let runningTotal = 0;

    for (const chunk of allChunks) {
      const tokens = estimateTokenCount(chunk);
      if (runningTotal + tokens > config.insightLLM.maxPromptTokens) break;

      selectedChunks.push(chunk);
      runningTotal += tokens;
    }

    return {
      chunks: selectedChunks,
      stats: {
        chunkCount: selectedChunks.length,
        tokenCount: runningTotal,
      },
    };
  }

  private async _generateCustomerEcosystem(
    context: string[],
    memoryContext?: string,
  ): Promise<string[]> {
    const messages = buildCustomerEcosystemPrompt(context, memoryContext);
    const response = await this.llm.generate({ messages });
    this.logger.log(`Customer Ecosystem: ${response.content}`);
    const extracted = extractSingleInsightSection(response.content, "customer_ecosystem");
    return extracted;
  }

  private async _generateCustomerValue(
    context: string[],
    memoryContext?: string,
  ): Promise<string[]> {
    const messages = buildCustomerValuePrompt(context, memoryContext);
    const response = await this.llm.generate({ messages });
    this.logger.log(`Customer Value: ${response.content}`);
    const extracted = extractSingleInsightSection(response.content, "customer_value");
    return extracted;
  }

  private async _generateCustomerInterests(
    context: string[],
    memoryContext?: string,
  ): Promise<string[]> {
    const messages = buildCustomerInterestsPrompt(context, memoryContext);
    const response = await this.llm.generate({ messages });
    this.logger.log(`Customer Interests: ${response.content}`);
    const extracted = extractSingleInsightSection(response.content, "customer_interests");
    return extracted;
  }

  private async _generateNextSteps(context: string[], memoryContext?: string): Promise<string[]> {
    const messages = buildNextStepsPrompt(context, memoryContext);
    const response = await this.llm.generate({ messages });
    this.logger.log(`Next Steps: ${response.content}`);
    const extracted = extractSingleInsightSection(response.content, "next_steps");
    return extracted;
  }
}
```

### Chunking Use Cases

```typescript
// From .references/c1-rsf-app/src/application/use-cases/chunk/account-summary-chunk.usecase.ts
import * as sbd from "sbd";

import { TAccountSummaryNormalizedData } from "@/application/dto/account.dto";
import { QdrantVectorStore } from "@/infrastructure/vector/vector-store.qdrant.service";
import { config } from "@/shared/config";
import { generateContentHash } from "@/shared/utils/hash-utils";
import { estimateTokenCount } from "@/shared/utils/token-utils";
import { BaseChunkBuilder } from "./base-chunk-builder";

export class AccountSummaryChunkUseCase extends BaseChunkBuilder {
  constructor(private readonly vectorStore = new QdrantVectorStore()) {
    super();
  }

  async run(accountData: TAccountSummaryNormalizedData): Promise<string[]> {
    const chunks = await this._chunkText(accountData.normalized);

    for (const [index, chunk] of chunks.entries()) {
      const contentHash = generateContentHash(chunk);
      const embedding = await this._safeGenerateEmbedding(chunk);

      if (!embedding.length) continue;

      await this.vectorStore.upsert({
        collectionName: config.qdrant.collection,
        id: this._generateChunkId(),
        embedding,
        payload: {
          content: chunk,
          contentHash,
          accountNumber: accountData.accountNumber,
          chunkIndex: index,
          gemStatus: accountData.gemStatus,
          topVendors: accountData.topVendors?.map((v) => v.name) ?? [],
          tokenCount: estimateTokenCount(chunk),
          sourceType: "account-summary",
          createdAt: new Date().toISOString(),
          embeddingModel: config.embeddingLLM.model,
          originalText: accountData.originalText,
        },
      });
    }

    return chunks;
  }

  protected override async _chunkText(text: string): Promise<string[]> {
    const maxTokens = config.insightLLM.maxTokensPerChunk;
    const overlapTokens = config.insightLLM.tokenOverlapPerChunk;

    const sentences = this._splitIntoSentences(text);
    const chunks: string[] = [];
    let current: string[] = [];

    for (let i = 0; i < sentences.length; i++) {
      current.push(sentences[i]);
      const joined = current.join(" ");
      const tokenCount = estimateTokenCount(joined);

      if (tokenCount > maxTokens) {
        const chunk = current.slice(0, -1).join(" ");
        if (chunk) chunks.push(chunk);

        // slice enough sentences to approximate overlapTokens
        let overlapStart = current.length - 1;
        let overlapTokenCount = 0;
        while (overlapStart > 0 && overlapTokenCount < overlapTokens) {
          overlapTokenCount += estimateTokenCount(current[overlapStart]);
          overlapStart--;
        }

        current = current.slice(overlapStart);
      }
    }

    if (current.length > 0) {
      chunks.push(current.join(" "));
    }

    return chunks;
  }

  private _splitIntoSentences(text: string): string[] {
    return sbd.sentences(text, {
      newline_boundaries: true,
      html_boundaries: true,
      sanitize: true,
      allowed_tags: false,
      abbreviations: ["e.g", "i.e", "U.S", "U.K", "Mr", "Mrs", "Dr"],
    });
  }
}
```

---

## 2.5 RSF Data Processing Flows

This section illustrates how data flows through the RSF application from import to insight generation.

### Overall System Architecture

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           RSF Data Processing Pipeline                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  INPUT SOURCES           IMPORT LAYER            STORAGE LAYER              │
│  ┌──────────┐          ┌──────────────┐       ┌──────────────┐            │
│  │ CSV Files│ ──────→  │ Import       │ ────→ │  PostgreSQL  │            │
│  │  - Accounts         │ Use Cases    │       │  Database    │            │
│  │  - Products         │              │       │              │            │
│  │  - Opportunities    │              │       │              │            │
│  └──────────┘          └──────────────┘       └──────────────┘            │
│                                                       ↓                     │
│  ┌──────────┐          ┌──────────────┐       ┌──────────────┐            │
│  │Doc Files │ ──────→  │ File Parser  │ ────→ │   Qdrant     │            │
│  │ - PPTX              │ Service      │       │Vector Store  │            │
│  │ - PDF               │              │       │              │            │
│  │ - DOCX              │              │       │              │            │
│  └──────────┘          └──────────────┘       └──────────────┘            │
│                                                                             │
│  PROCESSING LAYER                              OUTPUT LAYER                 │
│  ┌──────────────┐     ┌──────────────┐       ┌──────────────┐            │
│  │Normalization │ ←→  │    LLM       │ ────→ │   Insights   │            │
│  │  Services    │     │  Services    │       │   Export     │            │
│  └──────────────┘     └──────────────┘       └──────────────┘            │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Import Flows

#### 1. Account Import Flow

```
CLI Command: npm run import:accounts
│
├→ import-accounts.ts
│  └→ ImportAccountsUseCase
│     ├→ AccountLoaderService.loadMergedAccounts()
│     │  ├→ Load GEM accounts CSV
│     │  ├→ Load CRM accounts CSV
│     │  └→ Merge data by account number
│     │
│     └→ For each account:
│        └→ prisma.account.upsert()
│           ├→ Create if new
│           └→ Update if exists
│
Output: Accounts stored in PostgreSQL
```

#### 2. Product Import Flow

```
CLI Command: npm run import:products
│
├→ import-products.ts
│  └→ ImportProductsUseCase
│     ├→ ProductLoaderService.loadAll()
│     │  └→ Parse products CSV (10k+ rows)
│     │
│     └→ Batch processing (10,000 per batch):
│        └→ prisma.product.createMany()
│           └→ skipDuplicates: true
│
Output: Products stored in PostgreSQL
```

#### 3. Asset Import Flow

```
CLI Command: npm run import:assets
│
├→ import-assets.ts
│  └→ ImportAssetsUseCase
│     ├→ AssetLoaderService.load()
│     │  └→ Parse asset metadata Excel file
│     │
│     └→ For each asset file:
│        ├→ FileParserService.parse()
│        │  ├→ PPTX → slides
│        │  ├→ PDF → paragraphs
│        │  ├→ DOCX → sections
│        │  └→ XLSX → rows
│        │
│        ├→ DocumentNormalizerService.normalize()
│        │  └→ Convert to standardized chunks
│        │
│        ├→ RecursiveCharacterTextSplitter.split()
│        │  └→ Split by token limits (400/200)
│        │
│        └→ GlobalVectorChunkBuilder.run()
│           ├→ Generate embeddings (Ollama)
│           └→ Store in Qdrant vector DB
│
Output: Document chunks in vector store
```

#### 4. Opportunity Import Flow

```
CLI Command: npm run import:opportunities
│
├→ import-opportunities.ts
│  └→ ImportOpportunitiesUseCase
│     ├→ Load all accounts & products
│     │
│     ├→ OpportunityLoaderService.loadPurchases()
│     │  ├→ Parse opportunities CSV
│     │  └→ Parse purchase items CSV
│     │
│     ├→ For each opportunity:
│     │  └→ prisma.opportunity.upsert()
│     │
│     └→ For each purchase item:
│        └→ prisma.purchaseProduct.create()
│           └→ Link opportunity ↔ product
│
Output: Opportunities & purchase links in PostgreSQL
```

### Processing Flows

#### 5. Account Summary Normalization Flow

```
CLI Command: npm run chunk:account-summaries
│
├→ chunk-account-summaries.ts
│  └→ For each account:
│     ├→ AccountDataService.getAccountInsightData()
│     │  ├→ Aggregate opportunities
│     │  ├→ Calculate revenue metrics
│     │  ├→ Extract top vendors/products
│     │  └→ Generate account summary
│     │
│     ├→ AccountSummaryNormalizerService.normalize()
│     │  ├→ Format data for LLM
│     │  └→ Use NormalizationLLMService
│     │     └→ Temperature: 0.4 (consistent)
│     │
│     └→ AccountSummaryChunkUseCase.run()
│        ├→ Split into chunks (sbd library)
│        ├→ Generate embeddings
│        └→ Store in Qdrant with metadata:
│           ├→ accountNumber
│           ├→ gemStatus
│           ├→ topVendors[]
│           └→ sourceType: "account-summary"
│
Output: Normalized account chunks in vector store
```

#### 6. Document Chunking & Vectorization Flow

```
Generic document processing pipeline:
│
├→ Parse document → TParsedBlock[]
│  ├→ title
│  ├→ bullets[]
│  └→ slideNumber/sectionNumber
│
├→ Normalize content → TNormalizedChunk[]
│  ├→ content
│  ├→ originalText
│  ├→ tokenCount
│  └→ metadata
│
├→ Generate embeddings
│  └→ OllamaEmbeddingService
│     └→ /v1/embeddings API
│
└→ Store in Qdrant
   └→ QdrantVectorStore.upsert()
      ├→ id (UUID)
      ├→ vector (embedding)
      └→ payload (metadata)
```

### Insight Generation Flow

#### 7. Complete Insight Generation Pipeline

```
CLI Command: npm run generate:insights
│
├→ generate-insight-batch.ts
│  └→ For each account:
│     └→ GenerateInsightUseCase.run()
│        │
│        ├→ [1] Prepare Context
│        │  ├→ Load account data
│        │  └→ Combine: originalText + normalizedSummary
│        │
│        ├→ [2] Vector Search
│        │  └→ QdrantVectorSearchService.fetchRelevantChunks()
│        │     ├→ Account chunks (limit: 5)
│        │     │  └→ Filter: accountNumber match
│        │     └→ Solution chunks (limit: 10)
│        │        └→ Filter: scope = "sales-assets"
│        │
│        ├→ [3] Parallel Insight Generation
│        │  ├→ buildCustomerEcosystemPrompt() → InsightLLMService
│        │  ├→ buildCustomerValuePrompt() → InsightLLMService
│        │  ├→ buildCustomerInterestsPrompt() → InsightLLMService
│        │  └→ All use temperature: 0.7
│        │
│        ├→ [4] Sequential Next Steps
│        │  └→ buildNextStepsPrompt()
│        │     └→ Input: all previous insights + chunks
│        │
│        └→ [5] Format Output
│           └→ InsightResultDTO
│              ├→ accountNumber
│              ├→ generatedAt
│              └→ categories:
│                 ├→ customer_ecosystem[]
│                 ├→ customer_value[]
│                 ├→ customer_interests[]
│                 └→ next_steps[]
│
Output: JSON insights file per account
```

### Export/Query Flows

#### 8. Vector Search Flow

```
Query Processing:
│
├→ Input query text
│  └→ OllamaEmbeddingService.generateEmbedding()
│
├→ QdrantClient.search()
│  ├→ Vector similarity search
│  ├→ Apply filters (accountNumber, scope, etc.)
│  └→ Return top K results
│
└→ Extract content from payloads
   └→ Return relevant text chunks
```

#### 9. Evaluation & Benchmarking Flow

```
CLI Command: npm run evaluate:query-benchmarks
│
├→ Load benchmark queries
│  └→ TEvaluationQueryBenchmark[]
│
├→ For each query:
│  ├→ Execute vector search
│  ├→ Compare with expected results
│  └→ Calculate metrics:
│     ├→ Recall@K
│     ├→ MRR (Mean Reciprocal Rank)
│     └→ Precision scores
│
└→ Generate evaluation report
```

### Data Dependencies & Sequencing

```
Import Order (Required):
1. Products → (independent)
2. Accounts → (independent)
3. Opportunities → (requires Products & Accounts)
4. Assets → (independent)

Processing Order:
1. Import all data
2. Chunk account summaries
3. Chunk opportunity details (optional)
4. Import & chunk assets
5. Generate insights

Key Services:
- FileParserService: Multi-format document parsing
- NormalizationLLMService: Consistent formatting (temp: 0.4)
- InsightLLMService: Creative insights (temp: 0.7)
- QdrantVectorStore: Embedding storage & retrieval
- AccountDataService: Complex aggregations & metrics
```

---

## 3. LLM & Vector Integration

### Flowise API Client (from E4E)

```typescript
// From .references/c1-e4e-app/src/lib/flowise-api.ts
"use server";

import type { AssistantStyle, AssistantTone, AssistantType, User } from "@/lib/prisma";
import * as Sentry from "@sentry/nextjs";
import { FlowiseClient } from "flowise-sdk";
import type { StreamResponse } from "flowise-sdk/dist/flowise-sdk";
import { getEnvVariable } from "./utils";
type FlowiseChatResponse = AsyncGenerator<StreamResponse, void, unknown>;

interface FlowiseChatRequest {
  chatId: string;
  user: User;
  query: string;
  preferences: { tone: AssistantTone; style: AssistantStyle };
  assistant: AssistantType;
}

const assistantFlowMap: Record<AssistantType, { baseUrl: string; flowId: string }> = {
  GENERAL: {
    baseUrl: getEnvVariable("FLOWISE_CHAT_GENERAL_BASE_URL"),
    flowId: getEnvVariable("FLOWISE_CHAT_GENERAL_FLOW_ID"),
  },
  RESEARCH: {
    baseUrl: getEnvVariable("FLOWISE_CHAT_RESEARCH_BASE_URL"),
    flowId: getEnvVariable("FLOWISE_CHAT_RESEARCH_FLOW_ID"),
  },
  REASON: {
    baseUrl: getEnvVariable("FLOWISE_CHAT_REASON_BASE_URL"),
    flowId: getEnvVariable("FLOWISE_CHAT_REASON_FLOW_ID"),
  },
  HUMAN_RESOURCES: {
    baseUrl: getEnvVariable("FLOWISE_CHAT_HR_BASE_URL"),
    flowId: getEnvVariable("FLOWISE_CHAT_HR_FLOW_ID"),
  },
  RFP: {
    baseUrl: getEnvVariable("FLOWISE_CHAT_RFP_BASE_URL"),
    flowId: getEnvVariable("FLOWISE_CHAT_RFP_FLOW_ID"),
  },
  MEETING: {
    baseUrl: getEnvVariable("FLOWISE_MEETING_CHAT_BASE_URL"),
    flowId: getEnvVariable("FLOWISE_MEETING_CHAT_FLOW_ID"),
  },
  ENHANCE_PROMPT: {
    baseUrl: getEnvVariable("FLOWISE_CHAT_ENHANCE_PROMPT_BASE_URL"),
    flowId: getEnvVariable("FLOWISE_CHAT_ENHANCE_PROMPT_FLOW_ID"),
  },
};

class FlowiseApi {
  static async getFlow({
    chatId,
    user,
    query,
    assistant,
    preferences,
  }: FlowiseChatRequest): Promise<FlowiseChatResponse> {
    try {
      const { baseUrl, flowId } = assistantFlowMap[assistant];

      const client = new FlowiseClient({ baseUrl, apiKey: getEnvVariable("FLOWISE_AUTH_TOKEN") });

      return await client.createPrediction({
        chatflowId: flowId,
        question: query,
        streaming: true,
        overrideConfig: {
          sessionId: chatId,
          vars: { flow: assistant, style: preferences.style, tone: preferences.tone },
        },
        analytics: { langFuse: { userId: user.oid } },
      });
    } catch (error) {
      Sentry.withScope((scope) => {
        scope.setExtra("error", error);
        Sentry.captureMessage("Failed to create prediction", { level: "error" });
      });

      throw new Error("Failed to create prediction");
    }
  }
}

export { FlowiseApi };
```

### LLM Services (from RSF)

```typescript
// From .references/c1-rsf-app/src/infrastructure/llm/insight-llm.service.ts
import {
  ILanguageModelService,
  TLanguageModelParams,
} from "@/application/interfaces/language-model.service.interface";
import { config } from "@/shared/config";
import { TAIMessage } from "@/types";
import { LoggerService } from "../log/logger.service";

export class InsightLLMService implements ILanguageModelService {
  private readonly baseUrl = config.insightLLM.serverUrl;
  private readonly model = config.insightLLM.model;
  private readonly logger = new LoggerService("insight", "insights.json");

  async generate(params: TLanguageModelParams): Promise<TAIMessage> {
    const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        ...(config.insightLLM.apiKey && {
          Authorization: `Bearer ${config.insightLLM.apiKey}`,
        }),
      },
      body: JSON.stringify({
        model: this.model,
        messages: params.messages,
        temperature: 0.7,
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`[Insight LLM] Failed: ${response.status} ${errorText}`);
    }
    const json = await response.json();

    await this.logger.log(JSON.stringify(json, null, 2));

    const result = json.choices?.[0]?.message?.content;

    return {
      role: "assistant",
      content: result?.trim() ?? "",
    };
  }
}

// From .references/c1-rsf-app/src/infrastructure/llm/normalization-llm.service.ts
import {
  ILanguageModelService,
  TLanguageModelParams,
} from "@/application/interfaces/language-model.service.interface";
import { config } from "@/shared/config";
import { TAIMessage } from "@/types";

export class NormalizationLLMService implements ILanguageModelService {
  private readonly baseUrl = config.normalizationLLM.serverUrl;
  private readonly model = config.normalizationLLM.model;

  async generate(params: TLanguageModelParams): Promise<TAIMessage> {
    const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        ...(config.normalizationLLM.apiKey && {
          Authorization: `Bearer ${config.normalizationLLM.apiKey}`,
        }),
      },
      body: JSON.stringify({
        model: this.model,
        messages: params.messages,
        temperature: 0.4,
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`[Normalization LLM] Failed: ${response.status} ${errorText}`);
    }

    const json = await response.json();
    const result = json.choices?.[0]?.message?.content;

    return {
      role: "assistant",
      content: result?.trim() ?? "",
    };
  }
}
```

### Vector Store Services (from RSF)

```typescript
// From .references/c1-rsf-app/src/infrastructure/vector/vector-store.qdrant.service.ts
import { QdrantClient } from "@qdrant/js-client-rest";

import { config } from "@/shared/config";

export class QdrantVectorStore {
  private readonly client = new QdrantClient({ url: config.qdrant.url });

  constructor() {
    this._initCollection(); // Optional startup safety
  }

  private async _initCollection(): Promise<void> {
    const collections = await this.client.getCollections();
    const exists = collections.collections.some((c) => c.name === config.qdrant.collection);

    if (!exists) {
      await this.client.createCollection(config.qdrant.collection, {
        vectors: {
          size: Number(config.qdrant.vectorDimensions),
          distance: config.qdrant.distanceMetric,
        },
      });

      console.log(`🆕 Created Qdrant collection: ${config.qdrant.collection}`);
    }
  }

  async upsert(params: { collectionName: string; id: string; embedding: number[]; payload: any }) {
    if (!params.embedding || !Array.isArray(params.embedding) || params.embedding.length === 0) {
      throw new Error(`[QdrantVectorStore] Embedding is missing or invalid for ID: ${params.id}`);
    }

    await this.client.upsert(params.collectionName, {
      points: [
        {
          id: params.id,
          vector: params.embedding,
          payload: params.payload,
        },
      ],
    });
  }
}

// From .references/c1-rsf-app/src/infrastructure/vector/vector-search.qdrant.service.ts
import { QdrantClient } from "@qdrant/js-client-rest";

import { OllamaEmbeddingService } from "@/infrastructure/embedding/embedding.ollama.service";
import { config } from "@/shared/config";
import {
  TVectorMustFilter,
  TVectorScope,
  TVectorSearchScoredPoint,
  TVectorSourceType,
} from "@/types/vector";

export class QdrantVectorSearchService {
  private readonly client = new QdrantClient({ url: config.qdrant.url });
  private readonly embedder = new OllamaEmbeddingService();

  async getById(id: string): Promise<TVectorSearchScoredPoint | null> {
    const results = await this.client.retrieve(config.qdrant.collection, {
      with_payload: true,
      ids: [id.trim()],
      with_vector: true,
    });

    return results.map((p) => ({
      ...p,
      version: Number(p.order_value ?? 0),
      score: Number(p.order_value ?? 0),
    }))[0];
  }

  async getAllDocuments(params: { scope: string }): Promise<TVectorSearchScoredPoint[]> {
    const results = await this.client.scroll(config.qdrant.collection, {
      filter: {
        must: [{ key: "scope", match: { value: params.scope } }],
      },
      with_payload: true,
      with_vector: false,
    });

    return results.points.map((p) => ({
      ...p,
      version: Number(p.order_value ?? 0),
      score: Number(p.order_value ?? 0),
    }));
  }

  async fetchRelevantChunks(params: {
    accountNumber?: string;
    query: string;
    scope?: TVectorScope;
    sourceType?: TVectorSourceType;
    accessType?: string;
    limit?: number;
  }): Promise<string[]> {
    const { accountNumber, query, scope, sourceType, accessType, limit = 5 } = params;
    const embedding = await this.embedder.generateEmbedding(query);

    const mustFilters: TVectorMustFilter = [];

    if (accountNumber) {
      mustFilters.push({ key: "accountNumber", match: { value: accountNumber } });
    }

    if (scope) {
      mustFilters.push({ key: "scope", match: { value: scope } });
    }

    if (sourceType) {
      mustFilters.push({ key: "sourceType", match: { value: sourceType } });
    }

    if (accessType) {
      mustFilters.push({ key: "accessType", match: { value: accessType } });
    }

    const results = await this.client.search(config.qdrant.collection, {
      vector: embedding,
      limit,
      filter: {
        must: mustFilters,
      },
    });

    return results.map((res) => String(res.payload?.content ?? ""));
  }

  async searchWithEmbedding(params: {
    embedding: number[];
    topK: number;
    scope: string;
  }): Promise<TVectorSearchScoredPoint[]> {
    const results = await this.client.search(config.qdrant.collection, {
      vector: params.embedding,
      limit: params.topK,
      filter: {
        must: [{ key: "scope", match: { value: params.scope } }],
      },
      with_payload: true,
      with_vector: false,
    });

    return results.map((res) => ({
      ...res,
      version: Number(res.order_value ?? 0),
      score: Number(res.score ?? 0),
    }));
  }
}
```

### Embedding Service (from RSF)

```typescript
// From .references/c1-rsf-app/src/infrastructure/embedding/embedding.ollama.service.ts
import { IEmbeddingService } from "@/application/interfaces/embedding.service.interface";
import { config } from "@/shared/config";

export class OllamaEmbeddingService implements IEmbeddingService {
  private readonly baseUrl = config.embeddingLLM.serverUrl;
  private readonly model = config.embeddingLLM.model;

  async generateEmbedding(input: string): Promise<number[]> {
    const response = await fetch(`${this.baseUrl}/v1/embeddings`, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        ...(config.embeddingLLM.apiKey && {
          Authorization: `Bearer ${config.embeddingLLM.apiKey}`,
        }),
      },
      body: JSON.stringify({
        model: this.model,
        input,
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`[Embedding] Failed to fetch: ${response.status} ${errorText}`);
    }

    const json = await response.json();
    const embedding = json?.data?.[0]?.embedding;

    if (!embedding || !Array.isArray(embedding)) {
      throw new Error(`[Embedding] Invalid or missing embedding: ${JSON.stringify(json)}`);
    }

    return embedding;
  }
}
```

---

## 4. File Processing & Import

### File Parser Service (from RSF)

```typescript
// From .references/c1-rsf-app/src/infrastructure/import/file-parser.service.ts
import fs from "node:fs";
import path from "node:path";

import { TParsedBlock } from "@/types";
import { CsvParser } from "./parsers/csv.parser";
import { DocParser } from "./parsers/doc.parser";
import { ExcelParser } from "./parsers/excel.parser";
import { PdfParser } from "./parsers/pdf.parser";
import { PptxParser } from "./parsers/pptx.parser";

export class FileParserService {
  private readonly csvParser = new CsvParser();
  private readonly excelParser = new ExcelParser();
  private readonly pdfParser = new PdfParser();
  private readonly docParser = new DocParser();
  private readonly pptxParser = new PptxParser();

  async listFilePaths(dir: string): Promise<string[]> {
    return fs.promises.readdir(dir).then((files) => files.map((file) => path.resolve(dir, file)));
  }

  async parseCsv<Row extends Record<string, string>>(filePath: string): Promise<Row[]> {
    return this.csvParser.parse<Row>(filePath);
  }

  async parseExcel<Row extends Record<string, unknown>>(filePath: string): Promise<Row[]> {
    return this.excelParser.parse<Row>(filePath);
  }

  async parse(filePath: string): Promise<TParsedBlock[]> {
    const ext = path.extname(filePath).toLowerCase();

    if (ext.includes("pptx")) {
      const slides = await this.pptxParser.parse(filePath);
      return slides.map((slide) => ({
        title: slide.title,
        bullets: slide.bullets,
        slideNumber: slide.slideNumber,
      }));
    }

    if (ext.includes("pdf")) {
      const { text } = await this.pdfParser.parse(filePath);
      return text.split("\n\n").map((para, i) => ({
        title: `Paragraph ${i + 1}`,
        bullets: [para.trim()],
        sectionNumber: i + 1,
        rawText: para.trim(),
      }));
    }

    // Additional parsers for DOCX, XLSX...
    throw new Error(`Unsupported file type: ${ext}`);
  }
}
```

### CSV Parser (from RSF)

```typescript
// From .references/c1-rsf-app/src/infrastructure/import/parsers/csv.parser.ts
import { parse as csvParse } from "csv-parse/sync";
import fs from "node:fs";

import { IFileParserService } from "@/application/interfaces/file-parser.service.interface";

export class CsvParser implements IFileParserService<Record<string, string>[]> {
  supports(ext: string): boolean {
    return ext === ".csv";
  }

  async parse<T = Record<string, string>>(filePath: string): Promise<T[]> {
    const content = fs.readFileSync(filePath, "utf-8");
    return csvParse(content, { columns: true, skip_empty_lines: true }) as T[];
  }
}
```

---

## 5. Prompt Engineering (from RSF)

### Customer Ecosystem Prompt (from RSF)

```typescript
// From .references/c1-rsf-app/src/shared/prompts/insights/customer-ecosystem.prompt.ts
import {
  plainTextBulletScaffold,
  plainTextCSVInstructions,
  plainTextOutputAnchor,
  plainTextOutputHeader,
} from "./shared-output-structure";

export const customerEcosystemPrompt = `
You are a strategic business consultant analyzing customer data. Your task is to generate high-impact, context-grounded insights focused on the customer's technology ecosystem and vendor landscape.

ACCOUNT DATA:
{{context}}

CONTEXT INFORMATION:
{{memory_context}}

${plainTextCSVInstructions}

Only propose relevant vendors, platforms, or technologies if:
- They align with the customer's evaluated solutions, strategic initiatives, or known technology needs.
- They are commonly used in similar architectures for this customer's industry or problem domain.
- They appear in the provided context.

Do not include vendors or technologies that are unrelated to the customer's stated or implied direction.

Generate exactly four bullet points that:
- Identify key platforms and technologies already in use.
- Recommend upgrades, integrations, or shifts based on gaps.
- Highlight strategic vendor relationships worth preserving or expanding.
- Reference recent sales activity, top vendors, and key purchased products when helpful.

${plainTextOutputHeader}
${plainTextOutputAnchor("CUSTOMER_ECOSYSTEM")}

${plainTextBulletScaffold}
`.trim();
```

### Prompt Builder (from RSF)

```typescript
// From .references/c1-rsf-app/src/shared/prompts/insights/insight-prompt-builder.ts
import { TAIMessage } from "@/types";
import { customerEcosystemPrompt } from "./customer-ecosystem.prompt";
import { customerInterestsPrompt } from "./customer-interests.prompt";
import { customerValuePrompt } from "./customer-value.prompt";
import { nextStepsPrompt } from "./next-steps.prompt";

function interpolatePrompt(template: string, context: string[], memoryContext?: string): string {
  return template
    .replace("{{context}}", context.join("\n\n"))
    .replace("{{memory_context}}", memoryContext || "None");
}

export function buildCustomerEcosystemPrompt(
  context: string[],
  memoryContext?: string,
): TAIMessage[] {
  const prompt = interpolatePrompt(customerEcosystemPrompt, context, memoryContext);
  return [
    { role: "system", content: "You are a strategic consultant providing ecosystem insights." },
    { role: "user", content: prompt },
  ];
}

export function buildCustomerValuePrompt(context: string[], memoryContext?: string): TAIMessage[] {
  const prompt = interpolatePrompt(customerValuePrompt, context, memoryContext);
  return [
    {
      role: "system",
      content:
        "You are a strategic consultant identifying growth opportunities and strategic value.",
    },
    { role: "user", content: prompt },
  ];
}

export function buildCustomerInterestsPrompt(
  context: string[],
  memoryContext?: string,
): TAIMessage[] {
  const prompt = interpolatePrompt(customerInterestsPrompt, context, memoryContext);
  return [
    {
      role: "system",
      content:
        "You are a strategic consultant identifying customer interests, risks, and engagement gaps.",
    },
    { role: "user", content: prompt },
  ];
}

export function buildNextStepsPrompt(context: string[], memoryContext?: string): TAIMessage[] {
  const prompt = interpolatePrompt(nextStepsPrompt, context, memoryContext);
  return [
    {
      role: "system",
      content:
        "You are a strategic consultant recommending immediate next actions for the account team.",
    },
    { role: "user", content: prompt },
  ];
}
```

---

## 6. Configuration & Environment

### RSF Configuration

```typescript
// From .references/c1-rsf-app/src/shared/config.ts
import dotenv from "dotenv";
dotenv.config();

function requireEnv(key: string): string {
  const value = process.env[key];
  if (!value) throw new Error(`Missing environment variable: ${key}`);
  return value;
}

function parseIntEnv(key: string): number {
  return parseInt(requireEnv(key), 10);
}

type TQdrantDistanceMetric = "Cosine" | "Euclid" | "Dot" | "Manhattan";

export const config = {
  utilityLLM: {
    serverUrl: requireEnv("LLM_UTILITY_SERVER_URL"),
    apiKey: requireEnv("LLM_UTILITY_API_KEY"),
    model: requireEnv("LLM_UTILITY_MODEL"),
    tokenLimit: parseIntEnv("LLM_UTILITY_TOKEN_LIMIT"),
    reserveForResponse: parseIntEnv("LLM_UTILITY_TOKEN_RESERVE_FOR_RESPONSE"),
    get maxPromptTokens() {
      return this.tokenLimit - this.reserveForResponse;
    },
  },
  insightLLM: {
    serverUrl: requireEnv("LLM_INSIGHT_SERVER_URL"),
    apiKey: requireEnv("LLM_INSIGHT_API_KEY"),
    model: requireEnv("LLM_INSIGHT_MODEL"),
    tokenLimit: parseIntEnv("LLM_INSIGHT_TOKEN_LIMIT"),
    maxTokensPerChunk: parseIntEnv("LLM_INSIGHT_MAX_TOKENS_PER_CHUNK"),
    tokenOverlapPerChunk: parseIntEnv("LLM_INSIGHT_TOKEN_OVERLAP_PER_CHUNK"),
  },
  embeddingLLM: {
    serverUrl: requireEnv("LLM_EMBEDDING_SERVER_URL"),
    apiKey: requireEnv("LLM_EMBEDDING_API_KEY"),
    model: requireEnv("LLM_EMBEDDING_MODEL"),
  },
  qdrant: {
    url: requireEnv("QDRANT_URL"),
    collection: requireEnv("QDRANT_COLLECTION"),
    vectorDimensions: parseIntEnv("QDRANT_VECTOR_DIMENSIONS"),
    distanceMetric: requireEnv("QDRANT_DISTANCE_METRIC") as TQdrantDistanceMetric,
    defaultTopK: parseIntEnv("QDRANT_DEFAULT_TOP_K"),
  },
};
```

### Dependencies Comparison

```json
// RSF Dependencies (Processing & AI)
{
  "@prisma/client": "^6.8.2",
  "@qdrant/js-client-rest": "^1.14.0",
  "csv-parse": "^5.6.0",
  "csv-writer": "^1.6.0",
  "gpt-tokenizer": "^2.9.0",
  "mammoth": "^1.9.0",
  "pdf-parse": "^1.1.1",
  "pptx2json": "^0.0.10",
  "sbd": "^1.0.19",
  "xlsx": "^0.18.5",
  "ora": "^8.2.0",
  "chalk": "^5.4.1"
}

// E4E Dependencies (UI & Streaming)
{
  "flowise-sdk": "^1.0.9",
  "@tanstack/react-query": "^5.66.0",
  "framer-motion": "^12.0.11",
  "sonner": "^1.7.4",
  "zustand": "^5.0.3",
  "react-markdown": "^9.0.3",
  "remark-gfm": "^4.0.0"
}

// Current Northstar Dependencies
{
  "@prisma/client": "^6.6.0",
  "@tanstack/react-query": "^5.66.0",
  "next-auth": "^5.0.0-beta.25",
  "zustand": "^5.0.3",
  "react-markdown": "^9.0.3"
}
```

---

## 7. UI Patterns & Components (from E4E)

### Streaming Hook (from E4E)

```typescript
// From .references/c1-e4e-app/src/features/assistants/hooks/use-general-ai-reply-stream.tsx
"use client";

import { streamAIResponseAction } from "@/features";
import type { ChatWithQueries, TUserQuery } from "@/lib/prisma";
import { useQueryClient } from "@tanstack/react-query";

type TStreamAIReplyParams = {
  chatId: string;
  query: TUserQuery;
};

type TStreamAIReplyResponse = {
  streamReply: (params: TStreamAIReplyParams) => Promise<string>;
};

export const useGeneralAIReplyStream = (): TStreamAIReplyResponse => {
  const queryClient = useQueryClient();

  const streamReply = async ({ chatId, query }: TStreamAIReplyParams): Promise<string> => {
    const stream = await streamAIResponseAction({ chatId, query: query.content });
    const reader = stream.getReader();
    let accumulatedResponse = "";

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;
      accumulatedResponse += value;

      queryClient.setQueryData<ChatWithQueries | undefined>(["chat", chatId], (prevChat) => {
        if (!prevChat) return prevChat;
        return {
          ...prevChat,
          queries: prevChat.queries.map((q) =>
            q.id === query.id ? { ...q, aiResponse: accumulatedResponse, completed: false } : q,
          ),
        };
      });
    }
    return accumulatedResponse;
  };

  return { streamReply };
};
```

### Table Component (from E4E)

```typescript
// From .references/c1-e4e-app/src/components/ui/table.tsx
import { cn } from "@/lib/utils";
import type { HTMLAttributes, TdHTMLAttributes, ThHTMLAttributes } from "react";
import { forwardRef } from "react";

const Table = forwardRef<HTMLTableElement, HTMLAttributes<HTMLTableElement>>(
  ({ className, ...props }, ref) => (
    <div className="relative w-full overflow-auto">
      <table ref={ref} className={cn("w-full caption-bottom text-sm", className)} {...props} />
    </div>
  )
);

const TableHeader = forwardRef<HTMLTableSectionElement, HTMLAttributes<HTMLTableSectionElement>>(
  ({ className, ...props }, ref) => (
    <thead ref={ref} className={cn("[&_tr]:border-b", className)} {...props} />
  )
);

const TableBody = forwardRef<HTMLTableSectionElement, HTMLAttributes<HTMLTableSectionElement>>(
  ({ className, ...props }, ref) => (
    <tbody ref={ref} className={cn("[&_tr:last-child]:border-0", className)} {...props} />
  )
);

const TableRow = forwardRef<HTMLTableRowElement, HTMLAttributes<HTMLTableRowElement>>(
  ({ className, ...props }, ref) => (
    <tr ref={ref} className={cn("border-b transition-colors hover:bg-muted/50 data-[state=selected]:bg-muted", className)} {...props} />
  )
);

const TableHead = forwardRef<HTMLTableCellElement, ThHTMLAttributes<HTMLTableCellElement>>(
  ({ className, ...props }, ref) => (
    <th ref={ref} className={cn("h-12 px-4 text-left align-middle font-medium text-muted-foreground", className)} {...props} />
  )
);

const TableCell = forwardRef<HTMLTableCellElement, TdHTMLAttributes<HTMLTableCellElement>>(
  ({ className, ...props }, ref) => (
    <td ref={ref} className={cn("p-4 align-middle", className)} {...props} />
  )
);

export { Table, TableBody, TableCell, TableHead, TableHeader, TableRow };
```

### State Management (from E4E)

```typescript
// From .references/c1-e4e-app/src/stores/ui-store.ts
import { create } from "zustand";

interface UiStore {
  notificationCount: number;
  setNotificationCount: (notificationCount: number) => void;
}

export const useUIStore = create<UiStore>((set) => ({
  notificationCount: 0,
  setNotificationCount: (notificationCount) => set({ notificationCount }),
}));
```

---

## 8. API & Service Patterns

### Service Architecture (from RSF)

```typescript
// From .references/c1-rsf-app/src/infrastructure/persistence/account-data.prisma.service.ts
import { TAccountInsightData } from "@/application/dto/account.dto";
import { IAccountDataService } from "@/application/interfaces/account-data.service.interface";
import { prisma } from "@/lib/prisma";
import {
  TAccountOpportunitiesWithProducts,
  TPrismaClient,
  TScoredEntity,
  TUpdateAccountNormalizedSummaryParams,
} from "@/types";

export class AccountDataService implements IAccountDataService {
  private readonly db: TPrismaClient;

  constructor() {
    this.db = prisma;
  }

  async getAccountWithOpportunitiesWithProducts(params: {
    accountNumber: string;
  }): Promise<TAccountOpportunitiesWithProducts | null> {
    const { accountNumber } = params;

    const account = await this.db.account.findUnique({
      where: { accountNumber },
      include: {
        opportunities: {
          include: {
            purchaseProducts: { include: { product: true } },
          },
        },
      },
    });

    return account;
  }

  async getAccountInsightData(params: {
    accountNumber: string;
  }): Promise<TAccountInsightData | null> {
    const { accountNumber } = params;

    const account = await this.db.account.findUnique({
      where: { accountNumber },
    });

    if (!account) return null;

    const opportunities = await this.db.opportunity.findMany({
      where: { accountId: account.id },
      include: {
        purchaseProducts: { include: { product: true } },
      },
    });

    const topVendors = this._extractTopVendors(opportunities);
    const topProducts = this._extractTopProducts(opportunities);
    const totalRevenue = this._calculateRevenue(opportunities);

    return {
      accountNumber: account.accountNumber,
      industry: account.ceCustomerSegment ?? undefined,
      gemStatus: account.gemStatus ?? undefined,
      focusSolution: account.targetSolutions ?? undefined,
      opportunityCount: opportunities.length,
      pipelineValue: opportunities.reduce((sum, o) => sum + (o.pipelineGrossRevenue ?? 0), 0),
      bookedRevenue: opportunities.reduce((sum, o) => sum + (o.bookedGrossRevenue ?? 0), 0),
      // ... aggregation logic
      account,
      opportunities,
      topVendors,
      topProducts,
      totalRevenue,
    };
  }

  private _extractTopProducts(opportunities: any[]): TScoredEntity[] {
    // Complex aggregation logic for scoring products by revenue/count
    const productMap: Record<string, TScoredEntity> = {};
    // ... implementation
    return Object.values(productMap)
      .sort((a, b) => b.totalAmount - a.totalAmount)
      .slice(0, 5);
  }
}
```

### Authentication (from E4E)

```typescript
// From .references/c1-e4e-app/src/app/(auth)/auth.ts
import { prisma } from "@/lib/prisma";
import { getEnvVariable } from "@/lib/utils";
import { PrismaAdapter } from "@auth/prisma-adapter";
import type { NextAuthConfig } from "next-auth";
import NextAuth from "next-auth";
import MicrosoftEntraIdProvider from "next-auth/providers/microsoft-entra-id";

const providers = [
  MicrosoftEntraIdProvider({
    clientId: getEnvVariable("AUTH_MICROSOFT_ENTRA_ID_ID"),
    clientSecret: getEnvVariable("AUTH_MICROSOFT_ENTRA_ID_SECRET"),
    issuer: `https://login.microsoftonline.com/${getEnvVariable("AUTH_MICROSOFT_ENTRA_ID_TENANT_ID")}/v2.0`,
  }),
];

export const authOptions: NextAuthConfig = {
  providers,
  adapter: PrismaAdapter(prisma),
  session: { strategy: "jwt" },
  callbacks: {
    async jwt({ token, account }) {
      if (account) {
        token.accessToken = account.access_token;
        token.refreshToken = account.refresh_token;
        token.accessTokenExpires = Number(Date.now() + (account.expires_in ?? 0) * 1000);
      }
      return token;
    },
    async session({ session, token }) {
      session.accessToken = token.accessToken as string;
      session.user.id = token.sub as string;
      return session;
    },
  },
  events: {
    async signIn(message) {
      if (message.user.email) {
        const user = await prisma.user.findUnique({
          where: { email: message.user.email },
        });
        if (user && message.profile?.oid && !user.oid) {
          await prisma.user.update({
            where: { id: user.id },
            data: { oid: message.profile.oid },
          });
        }
      }
    },
  },
};

export const { handlers, auth, signIn, signOut } = NextAuth(authOptions);
```

---

## 9. Integration Recommendations

### Required Database Schema Changes

1. **Add Processing Models**:

   ```prisma
   model ProcessingJob {
     id          String   @id @default(uuid())
     type        JobType  // IMPORT_ACCOUNTS, GENERATE_INSIGHTS, etc.
     status      JobStatus @default(PENDING)
     progress    Int       @default(0)
     totalItems  Int       @default(0)
     errorMessage String?
     userId      String
     user        User     @relation(fields: [userId], references: [id])
     createdAt   DateTime @default(now())
     updatedAt   DateTime @updatedAt
   }

   model Document {
     id          String    @id @default(uuid())
     fileName    String
     originalName String
     filePath    String
     fileType    String    // csv, pdf, pptx, docx, etc.
     fileSize    Int
     status      DocumentStatus @default(UPLOADED)
     userId      String
     user        User      @relation(fields: [userId], references: [id])
     createdAt   DateTime  @default(now())
     processedAt DateTime?
   }

   enum JobType {
     IMPORT_ACCOUNTS
     IMPORT_OPPORTUNITIES
     IMPORT_PRODUCTS
     IMPORT_ASSETS
     GENERATE_INSIGHTS
     CHUNK_DATA
   }

   enum JobStatus {
     PENDING
     RUNNING
     COMPLETED
     FAILED
     CANCELLED
   }
   ```

2. **Extend User Model**:

   ```prisma
   model User {
     // ... existing fields
     processingJobs ProcessingJob[]
     documents     Document[]
     preferences   Json? // Store UI preferences, LLM settings
   }
   ```

3. **Add Business Data Models** (from RSF):
   - Copy Account, Opportunity, Product, Engineer models
   - Add normalized summary and insight fields
   - Add vector search metadata

### API Endpoints to Create

1. **File Processing APIs**:

   ```typescript
   // POST /api/upload - File upload with validation
   // GET /api/jobs - List processing jobs for user
   // GET /api/jobs/[id] - Get job status and progress
   // POST /api/jobs/[id]/cancel - Cancel running job

   // POST /api/import/accounts - Import accounts from uploaded CSV
   // POST /api/import/opportunities - Import opportunities
   // POST /api/import/products - Import products
   // POST /api/import/assets - Import and parse documents
   ```

2. **Insight Generation APIs**:

   ```typescript
   // POST /api/insights/generate - Generate insights for account
   // GET /api/insights/[accountId] - Get cached insights
   // POST /api/insights/batch - Batch generate insights
   ```

3. **Vector Search APIs**:
   ```typescript
   // POST /api/search/chunks - Search document chunks
   // POST /api/search/accounts - Search account data
   // GET /api/search/similar/[chunkId] - Find similar chunks
   ```

### Dependencies to Add

```json
{
  "dependencies": {
    "@qdrant/js-client-rest": "^1.14.0",
    "csv-parse": "^5.6.0",
    "csv-writer": "^1.6.0",
    "gpt-tokenizer": "^2.9.0",
    "mammoth": "^1.9.0",
    "pdf-parse": "^1.1.1",
    "pptx2json": "^0.0.10",
    "sbd": "^1.0.19",
    "xlsx": "^0.18.5",
    "multer": "^1.4.5-lts.1",
    "@types/multer": "^1.4.11",
    "socket.io": "^4.7.5",
    "socket.io-client": "^4.7.5"
  }
}
```

### Components to Build

1. **File Management Components**:
   - `FileUploadDropzone` - Drag & drop file upload
   - `FileList` - Display uploaded files with status
   - `FileTypeIcon` - Icons for different file types
   - `ProcessingProgress` - Real-time progress bars

2. **Job Management Components**:
   - `JobQueue` - Display processing jobs
   - `JobStatus` - Status badges and progress indicators
   - `JobDetails` - Detailed job information modal
   - `JobActions` - Cancel, retry, download results

3. **Data Visualization Components**:
   - `InsightCard` - Display generated insights
   - `AccountSummary` - Account overview with metrics
   - `VendorChart` - Top vendors visualization
   - `OpportunityTimeline` - Opportunity progression

4. **Configuration Components**:
   - `LLMSettings` - Configure AI model parameters
   - `VectorSearchConfig` - Vector database settings
   - `ImportMapping` - CSV column mapping interface

### Configuration Setup

```env
# LLM Configuration
LLM_INSIGHT_SERVER_URL=http://localhost:11434
LLM_INSIGHT_API_KEY=optional
LLM_INSIGHT_MODEL=llama3.1:8b
LLM_INSIGHT_TOKEN_LIMIT=8192
LLM_INSIGHT_TOKEN_RESERVE_FOR_RESPONSE=1024
LLM_INSIGHT_MAX_TOKENS_PER_CHUNK=512
LLM_INSIGHT_TOKEN_OVERLAP_PER_CHUNK=128

# Embedding Configuration
LLM_EMBEDDING_SERVER_URL=http://localhost:11434
LLM_EMBEDDING_MODEL=nomic-embed-text
LLM_EMBEDDING_TOKEN_LIMIT=2048

# Vector Database
QDRANT_URL=http://localhost:6333
QDRANT_COLLECTION=northstar_vectors
QDRANT_VECTOR_DIMENSIONS=768
QDRANT_DISTANCE_METRIC=Cosine
QDRANT_DEFAULT_TOP_K=10

# File Processing
MAX_FILE_SIZE=50000000  # 50MB
ALLOWED_FILE_TYPES=csv,pdf,docx,pptx,xlsx
UPLOAD_DIR=./uploads
PROCESSING_TEMP_DIR=./temp

# Job Processing
MAX_CONCURRENT_JOBS=3
JOB_TIMEOUT_MINUTES=30
ENABLE_BACKGROUND_PROCESSING=true
```

---

## 10. Implementation Roadmap

### Phase 1: Database & Types Setup (Week 1-2)

**1.1 Schema Migration**

- [ ] Add ProcessingJob, Document, and enum types to Prisma schema
- [ ] Extend User model with processing relationships
- [ ] Add business data models (Account, Opportunity, Product) from RSF
- [ ] Run database migration and test schema changes
- [ ] Generate updated Prisma client

**1.2 Type System**

- [ ] Create TypeScript types for all new models
- [ ] Add DTOs for API requests/responses
- [ ] Create interfaces for services (IFileParser, ILLMService, etc.)
- [ ] Add union types for job statuses and file types

**1.3 Environment Setup**

- [ ] Add LLM and vector database environment variables
- [ ] Create config service similar to RSF pattern
- [ ] Set up local Qdrant instance for development
- [ ] Configure file upload directories and permissions

### Phase 2: Core Processing Integration (Week 3-5)

**2.1 File Processing Services**

- [ ] Implement FileParserService with CSV, PDF, DOCX parsers
- [ ] Create upload handling with Multer integration
- [ ] Add file validation and virus scanning
- [ ] Implement progress tracking for large files

**2.2 LLM Integration**

- [ ] Create LLM service classes (InsightLLM, EmbeddingLLM)
- [ ] Implement prompt builders and templates
- [ ] Add token counting and context management
- [ ] Create vector embedding and chunking services

**2.3 Job Processing System**

- [ ] Implement job queue with background processing
- [ ] Create job runners for each import type
- [ ] Add job status tracking and error handling
- [ ] Implement WebSocket for real-time updates

**2.4 Vector Search Integration**

- [ ] Set up Qdrant client and connection management
- [ ] Implement vector store and search services
- [ ] Create chunk management and retrieval
- [ ] Add similarity search and filtering

### Phase 3: UI Components & API (Week 6-8)

**3.1 API Routes**

- [ ] Create file upload API with progress tracking
- [ ] Implement job management APIs (CRUD, status)
- [ ] Add import APIs for each data type
- [ ] Create insight generation APIs
- [ ] Implement search and retrieval APIs

**3.2 UI Components**

- [ ] Build FileUploadDropzone with drag & drop
- [ ] Create JobQueue component with real-time updates
- [ ] Implement data tables for imported data
- [ ] Add insight visualization components
- [ ] Create configuration panels for LLM settings

**3.3 State Management**

- [ ] Extend UI store for job management
- [ ] Add React Query for API state management
- [ ] Implement optimistic updates for job status
- [ ] Create error boundary and notification system

**3.4 Real-time Features**

- [ ] Set up Socket.IO for live job updates
- [ ] Implement progress bars and status indicators
- [ ] Add notifications for job completion/failure
- [ ] Create live search with debouncing

### Phase 4: Testing & Deployment (Week 9-10)

**4.1 Testing**

- [ ] Unit tests for all service classes
- [ ] Integration tests for API endpoints
- [ ] Component tests for file upload and job management
- [ ] End-to-end tests for complete import workflows
- [ ] Load testing for large file processing

**4.2 Performance Optimization**

- [ ] Implement file streaming for large uploads
- [ ] Add caching for frequently accessed data
- [ ] Optimize database queries with proper indexing
- [ ] Implement rate limiting for API endpoints

**4.3 Production Setup**

- [ ] Configure production LLM endpoints
- [ ] Set up production Qdrant cluster
- [ ] Implement file storage (AWS S3 or similar)
- [ ] Add monitoring and logging (Sentry integration)
- [ ] Create backup and recovery procedures

**4.4 Documentation & Training**

- [ ] Create user documentation for file import process
- [ ] Document API endpoints and usage
- [ ] Create admin guide for system configuration
- [ ] Prepare training materials for end users

### Success Metrics

- [ ] Successfully import and process 1000+ accounts
- [ ] Generate insights in under 30 seconds per account
- [ ] Handle file uploads up to 50MB without timeout
- [ ] Maintain 99.9% uptime for processing jobs
- [ ] User satisfaction score >4.5/5 for interface usability
